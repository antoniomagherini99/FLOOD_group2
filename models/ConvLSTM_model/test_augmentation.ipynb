{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e3fcb7-8923-49bd-b31d-6a5b697bd1fe",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "In this notebook data augmentation is performed for improving the performances of the ConvLSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d429f01-cb6c-4f9e-93fb-cdd420a4f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\OneDrive\\Desktop\\TU Delft\\Q6\\Data Science and Artificial Intelligence for Engineers\\FLOOD_group2\\models\n",
      "c:\\Users\\anton\\OneDrive\\Desktop\\TU Delft\\Q6\\Data Science and Artificial Intelligence for Engineers\\FLOOD_group2\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%cd ..\n",
    "# move to the root directory of the git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3804b918-987c-4142-a291-f151d13a5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import joblib\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch.nn as nn\n",
    "\n",
    "# Enable interactive widgets in Jupyter Notebook\n",
    "%matplotlib widget\n",
    "\n",
    "from torchsummary import summary\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from models.ConvLSTM_model.ConvLSTM_pytorch.convlstm import ConvLSTM\n",
    "from models.ConvLSTM_model.ConvLSTM_pytorch.multistep_convlstm import MultiStepConvLSTM\n",
    "from models.ConvLSTM_model.train_eval import train_epoch_conv_lstm, evaluation_conv_lstm\n",
    "from pre_processing.encode_decode_csv import decode_from_csv\n",
    "from pre_processing.normalization import * \n",
    "from pre_processing.augmentation import *\n",
    "from post_processing.cool_animation import plot_animation\n",
    "from post_processing.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af29ea0d-b11b-40eb-839a-d38771803139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save path\n",
    "save_path = 'models/ConvLSTM_model/model_paths/multiconv_lstm_10hid_4lay_5ker_augmentation.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86b0c0f-152f-4c4c-a77c-ba13942fe54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1af24d-3162-4ffe-b5c5-5517de643b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = 'train_val'\n",
    "test1 = 'test1'\n",
    "test2 = 'test2'\n",
    "test3 = 'test3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c4531f0-ffee-4a6f-9e7b-88029f5b3ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored inputs Shape: torch.Size([80, 1, 4, 64, 64])\n",
      "Restored targets Shape: torch.Size([80, 48, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# training and validation dataset\n",
    "train_dataset = decode_from_csv(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12279abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiFixedRotation:\n",
    "    '''\n",
    "    Class that implements random rotations of the dataset at fixed angles\n",
    "    '''\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # random.seed(seed)\n",
    "        angle = random.choice(self.angles)\n",
    "        return transforms.functional.rotate(x, angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e62b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(train_dataset, angles=[90,180,270], p_hflip=0.5, full=True):\n",
    "    '''\n",
    "    Function for implementing data augmentation of inputs (DEM, X- and Y-Slope,\n",
    "    Water Depth, and Discharge).\n",
    "\n",
    "    Input: train_dataset = torch tensor, dataset with input variables\n",
    "           seed = int, number for keeping the same random choice for trasforming both inputs and outputs in the same way, \n",
    "                  default = 42 \n",
    "           angles = list of angle degrees for random rotation of the dataset, \n",
    "                    default = 90°, 180°, 270°\n",
    "           p_hflip = float, probability of horizontal flipping\n",
    "                     default = 0.5 \n",
    "           \n",
    "    Output: transformed_dataset = new dataset with augmented data,\n",
    "                                  if full = True returns the original and trasformed dataset concatenated together\n",
    "                                  if full = False returns only the trasformed dataset \n",
    "    '''\n",
    "    \n",
    "    # transformation pipeline with horizontal flip\n",
    "    transformation_pipeline = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=p_hflip)])  \n",
    "        # transforms.RandomVerticalFlip(p=p_hflip)\n",
    "    \n",
    "    # rotation with MultiFixedRotation class\n",
    "    fixed_rotation = MultiFixedRotation(angles)\n",
    "\n",
    "    n_samples = len(train_dataset)\n",
    "\n",
    "    # initialize lists needed for looping   \n",
    "    inputs = []\n",
    "    outputs = []\n",
    "\n",
    "    transformed_inputs = []\n",
    "    transformed_outputs = []\n",
    "\n",
    "    for idx in range(len(train_dataset)):\n",
    "        \n",
    "        inputs.append(train_dataset[idx][0])\n",
    "        outputs.append(train_dataset[idx][1])\n",
    "\n",
    "    # get sizes of each dimension\n",
    "    inputs_sizes = train_dataset[0][0].shape\n",
    "    outputs_sizes = train_dataset[0][1].shape\n",
    "    print(f'Inputs sizes: {inputs_sizes},\\n\\\n",
    "Outputs sizes: {outputs_sizes}\\n')\n",
    "\n",
    "    # stack lists\n",
    "    inputs_tensor = torch.stack(inputs)\n",
    "    outputs_tensor = torch.stack(outputs)\n",
    "\n",
    "    # dimension of inputs channels over which concatentate \n",
    "    # inputs_flat_dim = inputs_tensor.size()[2]\n",
    "    inputs_flat_dim = inputs_sizes[1]\n",
    "\n",
    "    flattened_inputs = inputs_tensor.flatten(0,1) #alternatively try .view(-1, 64, 64) \n",
    "    flattened_outputs = outputs_tensor.flatten(1,2)\n",
    "    concat = torch.cat([flattened_inputs, flattened_outputs], dim=1)\n",
    "\n",
    "    # transformed_concat = fixed_rotation(transformation_pipeline(concat))\n",
    "    transformed_concat = transformation_pipeline(concat)\n",
    "    rotate_concat = [] \n",
    "    for i in range(len(transformed_concat)):\n",
    "        rotate_concat.append(fixed_rotation(transformed_concat[i]))\n",
    "\n",
    "    # reshape the tensors to original dimensions\n",
    "    transformed_inputs = rotate_concat[:, :inputs_flat_dim, :, :].view(n_samples, inputs_sizes[0], \n",
    "                                                                            inputs_sizes[1], inputs_sizes[2], inputs_sizes[3]) \n",
    "    transformed_outputs = rotate_concat[:, inputs_flat_dim:, :, :].view(n_samples, outputs_sizes[0], \n",
    "                                                                            outputs_sizes[1], outputs_sizes[2], outputs_sizes[3])\n",
    "\n",
    "    # concatenate tensors\n",
    "    all_inputs = torch.cat([inputs_tensor, transformed_inputs])\n",
    "    all_outputs = torch.cat([outputs_tensor, transformed_outputs])\n",
    "\n",
    "    # create Dataset type\n",
    "    transformed_dataset = torch.utils.data.TensorDataset(all_inputs, all_outputs)\n",
    "    \n",
    "    if full==True:\n",
    "        print(f'The samples in the dataset before augmentation were {len(train_dataset)}\\n\\\n",
    "The samples in the dataset after augmentation are {len(transformed_dataset)}')\n",
    "    \n",
    "    if full==False:\n",
    "        transformed_dataset = torch.utils.data.TensorDataset(transformed_inputs, transformed_outputs)\n",
    "        warning_msg = (\n",
    "f'\\nBe careful, you are not including the transformed dataset into the original one!\\n\\\n",
    "The samples in the dataset before augmentation were {len(train_dataset)}\\n\\\n",
    "The samples in the dataset after augmentation are {len(transformed_dataset)}\\n\\\n",
    "You are now using only the transformed dataset.'\n",
    ")\n",
    "        warnings.warn(warning_msg, RuntimeWarning)\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98c77fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs sizes: torch.Size([1, 4, 64, 64]),\n",
      "Outputs sizes: torch.Size([48, 2, 64, 64])\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# apply augmentation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m transformed_dataset \u001b[38;5;241m=\u001b[39m \u001b[43maugmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_hflip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 66\u001b[0m, in \u001b[0;36maugmentation\u001b[1;34m(train_dataset, angles, p_hflip, full)\u001b[0m\n\u001b[0;32m     63\u001b[0m     rotate_concat\u001b[38;5;241m.\u001b[39mappend(fixed_rotation(transformed_concat[i]))\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# reshape the tensors to original dimensions\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m transformed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mrotate_concat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43minputs_flat_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mview(n_samples, inputs_sizes[\u001b[38;5;241m0\u001b[39m], \n\u001b[0;32m     67\u001b[0m                                                                         inputs_sizes[\u001b[38;5;241m1\u001b[39m], inputs_sizes[\u001b[38;5;241m2\u001b[39m], inputs_sizes[\u001b[38;5;241m3\u001b[39m]) \n\u001b[0;32m     68\u001b[0m transformed_outputs \u001b[38;5;241m=\u001b[39m rotate_concat[:, inputs_flat_dim:, :, :]\u001b[38;5;241m.\u001b[39mview(n_samples, outputs_sizes[\u001b[38;5;241m0\u001b[39m], \n\u001b[0;32m     69\u001b[0m                                                                         outputs_sizes[\u001b[38;5;241m1\u001b[39m], outputs_sizes[\u001b[38;5;241m2\u001b[39m], outputs_sizes[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# concatenate tensors\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# apply augmentation\n",
    "transformed_dataset = augmentation(train_dataset, p_hflip=0.5, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919513fc-bd71-4798-bb75-4e37fc692333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "train_percnt = 0.8\n",
    "train_size = int(train_percnt * len(transformed_dataset))\n",
    "val_size = len(transformed_dataset) - train_size\n",
    "train_set, val_set = random_split(transformed_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a2667-7567-4102-8d58-79d480deb414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs using training dataset\n",
    "scaler_x, scaler_y = scaler(train_set)\n",
    "\n",
    "normalized_train_dataset = normalize_dataset(train_set, scaler_x, scaler_y, train_val)\n",
    "normalized_val_dataset = normalize_dataset(val_set, scaler_x, scaler_y, train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save scalers for use in seperate notebooks on testing\n",
    "joblib.dump(scaler_x, 'models/ConvLSTM_model/scalers/scaler_x_augmentation.joblib')\n",
    "joblib.dump(scaler_y, 'models/ConvLSTM_model/scalers/scaler_y_augmentation.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = MultiStepConvLSTM(input_dim = normalized_train_dataset[0][0].shape[1],\n",
    "                          output_dim = normalized_train_dataset[0][1].shape[1], \n",
    "                          hidden_dim = 10, kernel_size = (5, 5), num_layers = 4,\n",
    "                          batch_first=True, bias=True, return_all_layers = False).to(device)\n",
    "\n",
    "# model = ConvLSTM(input_dim = normalized_train_dataset[0][0].shape[1], output_dim = normalized_train_dataset[0][1].shape[1], hidden_dim = 32, kernel_size = (3, 3),\n",
    "#                  num_layers = 48, batch_first=True, bias=True, return_all_layers = True).to(device)\n",
    "\n",
    "# return all layers has to be true to obtain all the outputs I think\n",
    "# num_layers refers to the number of cells and thus outputs\n",
    "# Number of outputs = 4 gates * hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ce74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_parameters}\")\n",
    "model_size_MB = num_parameters * 4 / (1024 ** 2)  # Assuming float32 precision\n",
    "print(f\"Model size: {model_size_MB:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c79e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.005\n",
    "batch_size = 4 # Only have 64 and 16 samples for training and validation, I think should be kept small, having issues where this only works if set to 1\n",
    "num_epochs = 10 #1_000\n",
    "\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(normalized_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(normalized_val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # Model training\n",
    "    train_loss = train_epoch_conv_lstm(model, train_loader, optimizer, device)\n",
    "\n",
    "    # Model validation\n",
    "    val_loss = evaluation_conv_lstm(model, val_loader, device)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "    \n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        count = 0\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    count += 1\n",
    "    \n",
    "    if epoch%5 == 0:\n",
    "        print(f\"Epoch: {epoch} \" +\n",
    "              f\"\\t Training loss: {train_loss: .2e} \" + \n",
    "              f\"\\t Validation loss: {val_loss: .2e} \" +\n",
    "              f\"\\t Best validation loss: {best_loss: .2e}\")\n",
    "    stop_count = 100\n",
    "    if count > stop_count:\n",
    "        print(f\"Loss Stagnated for {stop_count} epochs, early stopping initiated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(best_model)\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0e35d-3866-4ff6-9ecb-d7b6ee0914a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, 'ConvLSTM (+ Augmentation) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From FAT application\n",
    "fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "# specify which entry of the dataset to plot\n",
    "numb = 20\n",
    "inputs = train_set[numb][0][0]\n",
    "\n",
    "axs[0].imshow(inputs[0].cpu(), cmap='terrain', origin='lower')\n",
    "axs[0].set_title('DEM')\n",
    "\n",
    "axs[1].imshow(inputs[1].cpu(), cmap='RdBu', origin='lower')\n",
    "axs[1].set_title('Slope X')\n",
    "\n",
    "axs[2].imshow(inputs[2].cpu(), cmap='RdBu', origin='lower')\n",
    "axs[2].set_title('Slope Y')\n",
    "\n",
    "non_zero_indices = torch.nonzero(inputs[3].cpu())\n",
    "non_zero_row, non_zero_col = non_zero_indices[0][0].item(), non_zero_indices[0][1].item()\n",
    "axs[3].imshow(inputs[3].cpu(), cmap='binary', origin='lower')\n",
    "axs[3].set_title('Breach Location')\n",
    "axs[3].scatter(non_zero_col, non_zero_row, color='k', marker='x', s=100,\n",
    "                clip_on = False, clip_box = plt.gca().transData)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851122a-a7bb-4c20-bea8-57f6820d520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(numb, normalized_train_dataset, model, train_val,\n",
    "               scaler_x, scaler_y, device = device, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bffa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(normalized_train_dataset[numb][0][0][0].cpu(), cmap='terrain', origin='lower')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c19fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_sorted(normalized_train_dataset, train_val, scaler_x, scaler_y, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e3fab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
