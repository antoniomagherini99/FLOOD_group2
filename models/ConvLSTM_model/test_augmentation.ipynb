{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d429f01-cb6c-4f9e-93fb-cdd420a4f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\OneDrive\\Desktop\\TU Delft\\Q6\\Data Science and Artificial Intelligence for Engineers\\FLOOD_group2\\models\n",
      "c:\\Users\\anton\\OneDrive\\Desktop\\TU Delft\\Q6\\Data Science and Artificial Intelligence for Engineers\\FLOOD_group2\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%cd ..\n",
    "# move to the root directory of the git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3804b918-987c-4142-a291-f151d13a5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch.nn as nn\n",
    "\n",
    "# Enable interactive widgets in Jupyter Notebook\n",
    "%matplotlib widget\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from models.ConvLSTM_model.ConvLSTM_pytorch.convlstm import ConvLSTM\n",
    "from models.ConvLSTM_model.train_eval import train_epoch_conv_lstm, evaluation_conv_lstm\n",
    "from pre_processing.encode_decode_csv import decode_from_csv\n",
    "from pre_processing.normalization import * \n",
    "from pre_processing.augmentation import *\n",
    "from post_processing.cool_animation import plot_animation\n",
    "from post_processing.plots import plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29ea0d-b11b-40eb-839a-d38771803139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save path\n",
    "save_path = 'models/ConvLSTM_model/conv_lstm_4batch_16hidden_3kernel_augmentation.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86b0c0f-152f-4c4c-a77c-ba13942fe54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1af24d-3162-4ffe-b5c5-5517de643b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = 'train_val'\n",
    "test1 = 'test1'\n",
    "test2 = 'test2'\n",
    "test3 = 'test3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4531f0-ffee-4a6f-9e7b-88029f5b3ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored inputs Shape: torch.Size([80, 1, 4, 64, 64])\n",
      "Restored targets Shape: torch.Size([80, 48, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# training and validation dataset\n",
    "train_dataset = decode_from_csv(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb23874c-ea94-483c-9e7a-6f695f6e365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From FAT application\n",
    "# fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "# # specify which entry of the dataset to plot\n",
    "# numb = 0\n",
    "# inputs = train_dataset[numb][0][0]\n",
    "\n",
    "# axs[0].imshow(inputs[0].cpu(), cmap='terrain', origin='lower')\n",
    "# axs[0].set_title('DEM')\n",
    "\n",
    "# axs[1].imshow(inputs[1].cpu(), cmap='RdBu', origin='lower')\n",
    "# axs[1].set_title('Slope X')\n",
    "\n",
    "# axs[2].imshow(inputs[2].cpu(), cmap='RdBu', origin='lower')\n",
    "# axs[2].set_title('Slope Y')\n",
    "\n",
    "# non_zero_indices = torch.nonzero(inputs[3].cpu())\n",
    "# non_zero_row, non_zero_col = non_zero_indices[0][0].item(), non_zero_indices[0][1].item()\n",
    "# axs[3].imshow(inputs[3].cpu(), cmap='binary', origin='lower')\n",
    "# axs[3].set_title('Breach Location')\n",
    "# axs[3].scatter(non_zero_col, non_zero_row, color='k', marker='x', s=100,\n",
    "#                 clip_on = False, clip_box = plt.gca().transData)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "269f969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(train_dataset, range_t, p_hflip=0.5, p_vflip=0.5, full=True):\n",
    "    '''\n",
    "    Function for implementing data augmentation of inputs (DEM, X- and Y-Slope,\n",
    "    Water Depth, and Discharge).\n",
    "\n",
    "    Input: train_dataset = torch tensor, dataset with input variables\n",
    "           p_hflip, p_vflip = float, probability of horizontal and vertical flipping\n",
    "                              default = 0.5 for both\n",
    "           angles = angle degrees for dataset rotation, fixed at 0째, 90째, 180째, 270째\n",
    "    Output:\n",
    "    '''\n",
    "    # Define the transformation pipeline with horizontal and vertical flip\n",
    "    transformation_pipeline = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=p_hflip),\n",
    "        transforms.RandomVerticalFlip(p=p_vflip)])\n",
    "\n",
    "    # Apply the transformation to each sample in the original dataset\n",
    "    transformed_samples = [transformation_pipeline(sample) for sample in train_dataset]\n",
    "\n",
    "    # Unpack the tuples and create a new dataset with the transformed samples\n",
    "    transformed_tensors = [torch.stack(sample) for sample in zip(*transformed_samples)]\n",
    "    transformed_dataset = torch.utils.data.TensorDataset(*transformed_tensors)\n",
    "\n",
    "    # Concatenate the original dataset and the transformed dataset\n",
    "    full_dataset = torch.utils.data.ConcatDataset([train_dataset, transformed_dataset])\n",
    "\n",
    "    # Print the shapes of datasets\n",
    "    print(f\"Shape of train_dataset: {np.shape(train_dataset)}\")\n",
    "    print(f\"Shape of transformed_dataset: {np.shape(transformed_dataset)}\")\n",
    "    print(f\"Shape of full_dataset: {np.shape(full_dataset)}\")\n",
    "\n",
    "    return full_dataset if full else transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7a2667-7567-4102-8d58-79d480deb414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs using training dataset\n",
    "scaler_x, scaler_wd, scaler_q = scaler(train_dataset)\n",
    "\n",
    "normalized_train_dataset = normalize_dataset(train_dataset, scaler_x, scaler_wd, scaler_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca65c564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\fromnumeric.py:2009: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  result = asarray(a).shape\n",
      "C:\\Users\\anton\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\fromnumeric.py:2009: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = asarray(a).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(normalized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "835b90da-4b00-428e-8548-6cde5db98217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_dataset: (80, 2)\n",
      "Shape of transformed_dataset: (80, 2)\n",
      "Shape of full_dataset: (160, 2)\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = augmentation(normalized_train_dataset, range_t=len(normalized_train_dataset), p_hflip=0.5, p_vflip=0.5, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58317ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(transformed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f1825a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat = torch.utils.data.ConcatDataset([normalized_train_dataset, transformed_dataset], collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08345a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "919513fc-bd71-4798-bb75-4e37fc692333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "train_percnt = 0.8\n",
    "train_size = int(train_percnt * len(transformed_dataset))\n",
    "val_size = len(transformed_dataset) - train_size\n",
    "train_set, val_set = random_split(transformed_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c1ed9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x23d793ca160>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d89b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set[0][0].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff34c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = ConvLSTM(input_dim = train_set[0][0].shape[1], output_dim = train_set[0][1].shape[1], hidden_dim = 16, kernel_size = (3, 3),\n",
    "                 num_layers = train_set[0][1].shape[0] + 1, batch_first=True, bias=True, return_all_layers = True).to(device)\n",
    "# return all layers has to be true to obtain all the outputs I think\n",
    "# num_layers refers to the number of cells and thus outputs\n",
    "# Number of outputs = 4 gates * hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49d6172a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLSTM(\n",
       "  (conv2): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (cell_list): ModuleList(\n",
       "    (0): ConvLSTMCell(\n",
       "      (conv): Conv2d(20, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1-48): 48 x ConvLSTMCell(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c79e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 4 # Only have 64 and 16 samples for training and validation, I think should be kept small, having issues where this only works if set to 1\n",
    "num_epochs = 8_000\n",
    "\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6abaa0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_conv_lstm(model, loader, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train() # specifies that the model is in training mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        \n",
    "        # Model prediction\n",
    "        preds, _ = model(x)\n",
    "    \n",
    "        # concat over the dimension 1 (time steps)\n",
    "        list_preds = torch.cat(preds, dim=1)\n",
    "        # print(np.shape(y))\n",
    "        # print(np.shape(list_preds))\n",
    "        # MSE loss function\n",
    "        loss = nn.MSELoss()(list_preds[:, 1:], y) #hard-coded, 1st output removed -- needs a check\n",
    "        \n",
    "        losses.append(loss.cpu().detach())\n",
    "        \n",
    "        # Backpropagate and update weights\n",
    "        loss.backward()   # compute the gradients using backpropagation\n",
    "        optimizer.step()  # update the weights with the optimizer\n",
    "        optimizer.zero_grad(set_to_none=True)   # reset the computed gradients\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5eee89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 48, 2, 64, 64])\n",
      "torch.Size([4, 49, 2, 64, 64])\n",
      "torch.Size([4, 48, 2, 64, 64])\n",
      "torch.Size([4, 49, 2, 64, 64])\n",
      "torch.Size([4, 48, 2, 64, 64])\n",
      "torch.Size([4, 49, 2, 64, 64])\n",
      "torch.Size([4, 48, 2, 64, 64])\n",
      "torch.Size([4, 49, 2, 64, 64])\n",
      "torch.Size([4, 48, 2, 64, 64])\n",
      "torch.Size([4, 49, 2, 64, 64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_conv_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Model validation\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluation_conv_lstm(model, val_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[38], line 8\u001b[0m, in \u001b[0;36mtrain_epoch_conv_lstm\u001b[1;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[0;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Model prediction\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # Model training\n",
    "    train_loss = train_epoch_conv_lstm(model, train_loader, optimizer, device=device)\n",
    "\n",
    "    # Model validation\n",
    "    val_loss = evaluation_conv_lstm(model, val_loader, device=device)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "    \n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(f\"epoch: {epoch} \\t training loss: {train_loss: .2e} \\t validation loss: {val_loss: .2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(best_model)\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0e35d-3866-4ff6-9ecb-d7b6ee0914a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, 'ConvLSTM + Augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b11304-05a6-4f6e-a8e4-bd567d050ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(10, normalized_train_dataset, model, train_val,\n",
    "               scaler_x, scaler_wd, scaler_q, device = device, save = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
