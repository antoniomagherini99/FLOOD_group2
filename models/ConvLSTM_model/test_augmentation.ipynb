{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e3fcb7-8923-49bd-b31d-6a5b697bd1fe",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "In this notebook data augmentation is performed for improving the performances of the ConvLSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d429f01-cb6c-4f9e-93fb-cdd420a4f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/FLOOD_group2/models\n",
      "/workspace/FLOOD_group2\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%cd ..\n",
    "# move to the root directory of the git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3804b918-987c-4142-a291-f151d13a5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch.nn as nn\n",
    "\n",
    "# Enable interactive widgets in Jupyter Notebook\n",
    "%matplotlib widget\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from models.ConvLSTM_model.ConvLSTM_pytorch.convlstm import ConvLSTM\n",
    "from models.ConvLSTM_model.ConvLSTM_pytorch.multistep_convlstm import MultiStepConvLSTM\n",
    "from models.ConvLSTM_model.train_eval import train_epoch_conv_lstm, evaluation_conv_lstm\n",
    "from pre_processing.encode_decode_csv import decode_from_csv\n",
    "from pre_processing.normalization import * \n",
    "from pre_processing.augmentation import *\n",
    "from post_processing.cool_animation import plot_animation\n",
    "from post_processing.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af29ea0d-b11b-40eb-839a-d38771803139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save path\n",
    "save_path = 'models/ConvLSTM_model/conv_lstm_4batch_16hidden_3kernel_augmentation.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86b0c0f-152f-4c4c-a77c-ba13942fe54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1af24d-3162-4ffe-b5c5-5517de643b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = 'train_val'\n",
    "test1 = 'test1'\n",
    "test2 = 'test2'\n",
    "test3 = 'test3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c4531f0-ffee-4a6f-9e7b-88029f5b3ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored inputs Shape: torch.Size([80, 1, 4, 64, 64])\n",
      "Restored targets Shape: torch.Size([80, 48, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# training and validation dataset\n",
    "train_dataset = decode_from_csv(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb23874c-ea94-483c-9e7a-6f695f6e365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From FAT application\n",
    "# fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "# # specify which entry of the dataset to plot\n",
    "# numb = 0\n",
    "# inputs = train_dataset[numb][0][0]\n",
    "\n",
    "# axs[0].imshow(inputs[0].cpu(), cmap='terrain', origin='lower')\n",
    "# axs[0].set_title('DEM')\n",
    "\n",
    "# axs[1].imshow(inputs[1].cpu(), cmap='RdBu', origin='lower')\n",
    "# axs[1].set_title('Slope X')\n",
    "\n",
    "# axs[2].imshow(inputs[2].cpu(), cmap='RdBu', origin='lower')\n",
    "# axs[2].set_title('Slope Y')\n",
    "\n",
    "# non_zero_indices = torch.nonzero(inputs[3].cpu())\n",
    "# non_zero_row, non_zero_col = non_zero_indices[0][0].item(), non_zero_indices[0][1].item()\n",
    "# axs[3].imshow(inputs[3].cpu(), cmap='binary', origin='lower')\n",
    "# axs[3].set_title('Breach Location')\n",
    "# axs[3].scatter(non_zero_col, non_zero_row, color='k', marker='x', s=100,\n",
    "#                 clip_on = False, clip_box = plt.gca().transData)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "269f969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(train_dataset, range_t, p_hflip=0.5, p_vflip=0.5, full=True):\n",
    "    '''\n",
    "    Function for implementing data augmentation of inputs (DEM, X- and Y-Slope,\n",
    "    Water Depth, and Discharge).\n",
    "\n",
    "    Input: train_dataset = torch tensor, dataset with input variables\n",
    "           p_hflip, p_vflip = float, probability of horizontal and vertical flipping\n",
    "                              default = 0.5 for both\n",
    "           angles = angle degrees for dataset rotation, fixed at 0째, 90째, 180째, 270째\n",
    "    Output:\n",
    "    '''\n",
    "    # Define the transformation pipeline with horizontal and vertical flip\n",
    "    transformation_pipeline = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=p_hflip),\n",
    "        transforms.RandomVerticalFlip(p=p_vflip)])\n",
    "\n",
    "    # Apply the transformation to each sample in the original dataset\n",
    "    transformed_samples = [transformation_pipeline(sample) for sample in train_dataset]\n",
    "\n",
    "    # Unpack the tuples and create a new dataset with the transformed samples\n",
    "    transformed_tensors = [torch.stack(sample) for sample in zip(*transformed_samples)]\n",
    "    transformed_dataset = torch.utils.data.TensorDataset(*transformed_tensors)\n",
    "\n",
    "    # Concatenate the original dataset and the transformed dataset\n",
    "    full_dataset = torch.utils.data.ConcatDataset([train_dataset, transformed_dataset])\n",
    "\n",
    "    # Print the shapes of datasets\n",
    "    # print(f\"Shape of train_dataset: {np.shape(train_dataset)}\")\n",
    "    # print(f\"Shape of transformed_dataset: {np.shape(transformed_dataset)}\")\n",
    "    # print(f\"Shape of full_dataset: {np.shape(full_dataset)}\")\n",
    "\n",
    "    return full_dataset if full else transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c7a2667-7567-4102-8d58-79d480deb414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs using training dataset\n",
    "scaler_x, scaler_wd, scaler_q = scaler(train_dataset)\n",
    "\n",
    "normalized_train_dataset = normalize_dataset(train_dataset, scaler_x, scaler_wd, scaler_q, train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "835b90da-4b00-428e-8548-6cde5db98217",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = augmentation(normalized_train_dataset, range_t=len(normalized_train_dataset), p_hflip=0.5, p_vflip=0.5, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "919513fc-bd71-4798-bb75-4e37fc692333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "train_percnt = 0.8\n",
    "train_size = int(train_percnt * len(transformed_dataset))\n",
    "val_size = len(transformed_dataset) - train_size\n",
    "train_set, val_set = random_split(transformed_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff34c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = MultiStepConvLSTM(input_dim = train_set[0][0].shape[1], output_dim = train_set[0][1].shape[1], hidden_dim = 16, kernel_size = (3, 3),\n",
    "                 num_layers = 48, batch_first=True, bias=True, return_all_layers = True).to(device)\n",
    "# return all layers has to be true to obtain all the outputs I think\n",
    "# num_layers refers to the number of cells and thus outputs\n",
    "# Number of outputs = 4 gates * hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49d6172a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiStepConvLSTM(\n",
       "  (conv2): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (cell_list): ModuleList(\n",
       "    (0): ConvLSTMCell(\n",
       "      (conv): Conv2d(20, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1-47): 47 x ConvLSTMCell(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c79e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 4 # Only have 64 and 16 samples for training and validation, I think should be kept small, having issues where this only works if set to 1\n",
    "num_epochs = 50\n",
    "\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eee89bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.73 GiB of which 2.06 MiB is free. Process 3151542 has 11.45 GiB memory in use. Process 1593770 has 4.27 GiB memory in use. Of the allocated memory 4.08 GiB is allocated by PyTorch, and 5.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch_conv_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Model validation\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluation_conv_lstm(model, val_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/workspace/FLOOD_group2/models/ConvLSTM_model/train_eval.py:68\u001b[0m, in \u001b[0;36mtrain_epoch_conv_lstm\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     66\u001b[0m y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 68\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mobtain_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# MSE loss function\u001b[39;00m\n\u001b[1;32m     70\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(predictions, y)\n",
      "File \u001b[0;32m/workspace/FLOOD_group2/models/ConvLSTM_model/train_eval.py:46\u001b[0m, in \u001b[0;36mobtain_predictions\u001b[0;34m(model, input, device, steps)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_who \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiStepConvLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, steps, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Requires input at all time steps\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unbatch:\n\u001b[1;32m     49\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predictions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;66;03m# remove batch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/FLOOD_group2/models/ConvLSTM_model/ConvLSTM_pytorch/multistep_convlstm.py:159\u001b[0m, in \u001b[0;36mMultiStepConvLSTM.forward\u001b[0;34m(self, input_tensor, hidden_state)\u001b[0m\n\u001b[1;32m    157\u001b[0m output_inner \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[0;32m--> 159\u001b[0m     h, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_layer_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcur_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     output_inner\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m    163\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(output_inner, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/FLOOD_group2/models/ConvLSTM_model/ConvLSTM_pytorch/multistep_convlstm.py:46\u001b[0m, in \u001b[0;36mConvLSTMCell.forward\u001b[0;34m(self, input_tensor, cur_state)\u001b[0m\n\u001b[1;32m     44\u001b[0m combined_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(combined)\n\u001b[1;32m     45\u001b[0m cc_i, cc_f, cc_o, cc_g \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(combined_conv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcc_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(cc_f)\n\u001b[1;32m     48\u001b[0m o \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(cc_o)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.73 GiB of which 2.06 MiB is free. Process 3151542 has 11.45 GiB memory in use. Process 1593770 has 4.27 GiB memory in use. Of the allocated memory 4.08 GiB is allocated by PyTorch, and 5.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # Model training\n",
    "    train_loss = train_epoch_conv_lstm(model, train_loader, optimizer, device=device)\n",
    "\n",
    "    # Model validation\n",
    "    val_loss = evaluation_conv_lstm(model, val_loader, device=device)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "    \n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(f\"epoch: {epoch} \\t training loss: {train_loss: .2e} \\t validation loss: {val_loss: .2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(best_model)\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0e35d-3866-4ff6-9ecb-d7b6ee0914a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, 'ConvLSTM (+ Augmentation) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b11304-05a6-4f6e-a8e4-bd567d050ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_animation(10, normalized_train_dataset, model, train_val,\n",
    "#                scaler_x, scaler_wd, scaler_q, device = device, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e8043-84d1-44a6-b80b-23d60e320cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sorted(dataset, train_val, scaler_x, scaler_wd, scaler_q, model, device):\n",
    "    '''\n",
    "    Function for plotting the DEMs variation sorted in increasing order \n",
    "    of average loss (of Water Depth and Discharge)\n",
    "\n",
    "    Input: dataset = tensor, normalized dataset\n",
    "           train_val_test : str, Identifier of dictionary. Expects: 'train_val', 'test1', 'test2', 'test3'.\n",
    "           scaler_x, scaler_wd, scaler_q = scalers for inputs (x) and targets (water depth and discharge), created \n",
    "                                            with the scaler function \n",
    "    Output: None (plot)\n",
    "    '''\n",
    "    \n",
    "    # get inputs and outputs\n",
    "    # 1st sample, 2nd input(0)/target(1), 3rd time step, 4th features, 5th/6th pixels\n",
    "    \n",
    "    # input = dataset[0][0]\n",
    "    # target = dataset[0][1]\n",
    "    \n",
    "    n_samples = len(dataset)\n",
    "    n_features = dataset[0][1].shape[1]\n",
    "    n_pixels = dataset[0][1].shape[-1]\n",
    "\n",
    "    # initialize inputs and outputs\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        inputs.append(dataset[i][0])\n",
    "        targets.append(dataset[i][1])\n",
    "\n",
    "    # initialize denormalization of dataset\n",
    "    elevations = np.zeros((n_samples, n_pixels, n_pixels))\n",
    "    print(type(elevations))\n",
    "    # initialize losses\n",
    "    losses = torch.zeros((n_samples, n_features))\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        # denormalize dataset\n",
    "        elevations[i], water_depth, discharges = denormalize_dataset(inputs[i], targets[i], train_val, \n",
    "                                                            scaler_x, scaler_wd, scaler_q)\n",
    "        # make predictions\n",
    "        preds = obtain_predictions(model, inputs[i], device)\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            # compute MSE losses\n",
    "            losses[i, feature] = nn.MSELoss()(preds[:][feature], targets[i][:][feature]) # [:, feature]\n",
    "    elevations_tensor = torch.tensor(elevations)\n",
    "    print(type(elevations_tensor))\n",
    "    # compute average loss for sorting dataset\n",
    "    print(losses.shape)\n",
    "    # loss with water depth, improve with normalized\n",
    "    # avg_loss = torch.mean() (losses[0]+losses[1])/2\n",
    "\n",
    "    # # compute recall - improvement: add minimium threshold for recall (wd > 10 cm), need to denormalize targets and predictions\n",
    "    # ask scaler what 10 is and plot that scaler_wd.transform(0.10) - check\n",
    "    # recall, _, _ = confusion_mat(dataset, model, device)\n",
    "\n",
    "    # # sorting dataset\n",
    "    # elevation_sorted = sorted(elevation)\n",
    "    # sorted_indexes = [index for index, _ in elevation_sorted]\n",
    "    # wd_sorted, q_sorted = [water_depth[i] for i in sorted_indexes], [discharge[i] for i in sorted_indexes]\n",
    "    # sorted_recall = [recall[i] for i in sorted_indexes]\n",
    "\n",
    "    # elevation_var = np.std(elevation_sorted)\n",
    "    \n",
    "    # # plot \n",
    "    # fig, axes = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
    "    # fig.subplots_adjust(wspace=0.5)\n",
    "\n",
    "    # # create second y-axis for discharge scale\n",
    "    # ax1_2 = axes[1].twinx()\n",
    "\n",
    "    # axes[0].boxplot(sorted_indexes, elevation_var, label='DEM')\n",
    "    # axes[1].plot(sorted_indexes, wd_sorted, color='blue', label='water depth')\n",
    "    # ax1_2.plot(sorted_indexes, q_sorted, color='red', label='discharge')\n",
    "    # axes[2].scatter(sorted_indexes, sorted_recall, color='green', label='recall')\n",
    "\n",
    "    # for ax in axes:\n",
    "    #     ax.set_xlabel('Sample ID')\n",
    "    \n",
    "    # axes[0].set_ylabel('Normalized variation [-]')\n",
    "    # axes[1].set_ylabel('Normalized Water Depth loss [-]')\n",
    "    # ax1_2.set_ylabel('Normalized Discharge loss [-]')\n",
    "    # axes[2].set_ylabel('Recall [-]')\n",
    "\n",
    "    # axes[0].set_title('Normalized DEM variation [-]')\n",
    "    # axes[1].set_title('Normalized MSE loss [-]')\n",
    "    # axes[2].set_title('Recall [-]')\n",
    "\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sorted(normalized_train_dataset, train_val, scaler_x, scaler_wd, scaler_q, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1a9e5-1438-4321-a5cf-e38a087bf93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_dataset[0][0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
