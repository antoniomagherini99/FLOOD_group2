{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e3fcb7-8923-49bd-b31d-6a5b697bd1fe",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "In this notebook data augmentation is performed for improving the performances of the ConvLSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d429f01-cb6c-4f9e-93fb-cdd420a4f612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/FLOOD_group2/models\n",
      "/workspace/FLOOD_group2\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%cd ..\n",
    "# move to the root directory of the git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3804b918-987c-4142-a291-f151d13a5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch.nn as nn\n",
    "\n",
    "# Enable interactive widgets in Jupyter Notebook\n",
    "%matplotlib widget\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from models.ConvLSTM_model.ConvLSTM_pytorch.convlstm import ConvLSTM\n",
    "from models.ConvLSTM_model.ConvLSTM_pytorch.multistep_convlstm import MultiStepConvLSTM\n",
    "from models.ConvLSTM_model.train_eval import train_epoch_conv_lstm, evaluation_conv_lstm\n",
    "from pre_processing.encode_decode_csv import decode_from_csv\n",
    "from pre_processing.normalization import * \n",
    "from pre_processing.augmentation import *\n",
    "from post_processing.cool_animation import plot_animation\n",
    "from post_processing.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af29ea0d-b11b-40eb-839a-d38771803139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save path\n",
    "save_path = 'models/ConvLSTM_model/conv_lstm_4batch_16hidden_3kernel_augmentation.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86b0c0f-152f-4c4c-a77c-ba13942fe54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1af24d-3162-4ffe-b5c5-5517de643b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = 'train_val'\n",
    "test1 = 'test1'\n",
    "test2 = 'test2'\n",
    "test3 = 'test3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c4531f0-ffee-4a6f-9e7b-88029f5b3ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored inputs Shape: torch.Size([80, 1, 4, 64, 64])\n",
      "Restored targets Shape: torch.Size([80, 48, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# training and validation dataset\n",
    "train_dataset = decode_from_csv(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c77fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The samples in the dataset before augmentation were 80\n",
      "The samples in the dataset after augmentation are 160\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = augmentation(train_dataset, range_t=len(train_dataset), p_hflip=0.5, p_vflip=0.5, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bacb8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def augmentation2(train_dataset, range_t, p_hflip=0.5, p_vflip=0.5, full=True): #angles=fixed_angles, \n",
    "#     '''\n",
    "#     Function for implementing data augmentation of inputs (DEM, X- and Y-Slope, \n",
    "#     Water Depth and Discharge).\n",
    "\n",
    "#     Input: train_dataset = torch tensor, dataset with input variables\n",
    "#            p_hflip, p_vflip = float, probability of horizontal and vertical flipping\n",
    "#                               default = 0.5 for both\n",
    "#            angles = angle degrees for dataset rotation, fixed at 0째, 90째, 180째, 270째\n",
    "#     Output:  \n",
    "#     '''\n",
    "#     # implement transformation pipeline with horizontal and vertical flip and rotation of fixed angles\n",
    "#     transformation_pipeline = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(p=p_hflip),\n",
    "#     transforms.RandomVerticalFlip(p=p_vflip)]) \n",
    "#     #transforms.functional.rotate(train_dataset[i] for i in range(len(train_dataset)), RandomFixedRotation(angles))\n",
    "\n",
    "#     # transform dataset\n",
    "#     transformed_dataset = [transformation_pipeline(train_dataset[0]) for _ in range(range_t)]\n",
    "#     print(f' Shape of transformed dataset {np.shape(transformed_dataset)}')\n",
    "\n",
    "#     tensor_datasets = [train_dataset, transformed_dataset]\n",
    "\n",
    "#     # Extract individual tensors and stack them along a new dimension\n",
    "#     stacked_tensors = torch.stack([torch.stack((ds.tensors[0], ds.tensors[1]), dim=1) for ds in tensor_datasets], dim=0)\n",
    "#     print(f'Type of stacked tensor: {type(stacked_tensors)}')\n",
    "    \n",
    "#     return stacked_tensors if full==True else transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef47ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transf2 = augmentation2(train_dataset, range_t=len(train_dataset), p_hflip=0.5, p_vflip=0.5, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "919513fc-bd71-4798-bb75-4e37fc692333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "train_percnt = 0.8\n",
    "train_size = int(train_percnt * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c7a2667-7567-4102-8d58-79d480deb414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs using training dataset\n",
    "scaler_x, scaler_wd, scaler_q = scaler(train_set)\n",
    "\n",
    "normalized_train_dataset = normalize_dataset(train_set, scaler_x, scaler_wd, scaler_q, train_val)\n",
    "normalized_train_dataset = normalize_dataset(val_set, scaler_x, scaler_wd, scaler_q, train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff34c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = ConvLSTM(input_dim = normalized_train_dataset[0][0].shape[1], output_dim = normalized_train_dataset[0][1].shape[1], hidden_dim = 16, kernel_size = (3, 3),\n",
    "                 num_layers = 48, batch_first=True, bias=True, return_all_layers = True).to(device)\n",
    "# return all layers has to be true to obtain all the outputs I think\n",
    "# num_layers refers to the number of cells and thus outputs\n",
    "# Number of outputs = 4 gates * hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49d6172a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLSTM(\n",
       "  (conv2): Conv2d(16, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (cell_list): ModuleList(\n",
       "    (0): ConvLSTMCell(\n",
       "      (conv): Conv2d(20, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1-47): 47 x ConvLSTMCell(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c79e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 4 # Only have 64 and 16 samples for training and validation, I think should be kept small, having issues where this only works if set to 1\n",
    "num_epochs = 10_000\n",
    "\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eee89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 \t training loss:  4.95e-02 \t validation loss:  5.54e-02\n",
      "epoch: 20 \t training loss:  4.95e-02 \t validation loss:  5.54e-02\n",
      "epoch: 30 \t training loss:  4.95e-02 \t validation loss:  5.54e-02\n",
      "epoch: 40 \t training loss:  4.95e-02 \t validation loss:  5.54e-02\n",
      "epoch: 50 \t training loss:  4.94e-02 \t validation loss:  5.54e-02\n",
      "epoch: 60 \t training loss:  4.95e-02 \t validation loss:  5.54e-02\n",
      "epoch: 70 \t training loss:  4.44e-02 \t validation loss:  4.95e-02\n",
      "epoch: 80 \t training loss:  3.70e-02 \t validation loss:  4.20e-02\n",
      "epoch: 90 \t training loss:  3.23e-02 \t validation loss:  3.67e-02\n",
      "epoch: 100 \t training loss:  2.61e-02 \t validation loss:  3.04e-02\n",
      "epoch: 110 \t training loss:  2.37e-02 \t validation loss:  2.99e-02\n",
      "epoch: 120 \t training loss:  2.16e-02 \t validation loss:  2.73e-02\n",
      "epoch: 130 \t training loss:  2.25e-02 \t validation loss:  2.72e-02\n",
      "epoch: 140 \t training loss:  2.15e-02 \t validation loss:  2.86e-02\n",
      "epoch: 150 \t training loss:  1.96e-02 \t validation loss:  3.17e-02\n",
      "epoch: 160 \t training loss:  1.87e-02 \t validation loss:  2.56e-02\n",
      "epoch: 170 \t training loss:  1.73e-02 \t validation loss:  2.75e-02\n",
      "epoch: 180 \t training loss:  1.68e-02 \t validation loss:  2.74e-02\n",
      "epoch: 190 \t training loss:  1.64e-02 \t validation loss:  2.83e-02\n",
      "epoch: 200 \t training loss:  1.71e-02 \t validation loss:  2.74e-02\n",
      "epoch: 210 \t training loss:  1.52e-02 \t validation loss:  2.67e-02\n",
      "epoch: 220 \t training loss:  1.62e-02 \t validation loss:  2.89e-02\n",
      "epoch: 230 \t training loss:  1.57e-02 \t validation loss:  2.86e-02\n",
      "epoch: 240 \t training loss:  1.44e-02 \t validation loss:  2.78e-02\n",
      "epoch: 250 \t training loss:  1.34e-02 \t validation loss:  2.97e-02\n",
      "epoch: 260 \t training loss:  1.42e-02 \t validation loss:  2.77e-02\n",
      "epoch: 270 \t training loss:  1.29e-02 \t validation loss:  3.14e-02\n",
      "epoch: 280 \t training loss:  1.21e-02 \t validation loss:  3.19e-02\n",
      "epoch: 290 \t training loss:  1.21e-02 \t validation loss:  3.34e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_epoch_conv_lstm(model, train_loader, optimizer, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Model validation\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_conv_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     12\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m val_loss\n",
      "File \u001b[0;32m/workspace/FLOOD_group2/models/ConvLSTM_model/train_eval.py:94\u001b[0m, in \u001b[0;36mevaluation_conv_lstm\u001b[0;34m(model, loader, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     92\u001b[0m y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 94\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mobtain_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# MSE loss function\u001b[39;00m\n\u001b[1;32m     96\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(predictions, y)\n",
      "File \u001b[0;32m/workspace/FLOOD_group2/models/ConvLSTM_model/train_eval.py:41\u001b[0m, in \u001b[0;36mobtain_predictions\u001b[0;34m(model, input, device, steps)\u001b[0m\n\u001b[1;32m     38\u001b[0m     unbatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_who \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 41\u001b[0m     sample_list, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(sample_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_who \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiStepConvLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/FLOOD_group2/models/ConvLSTM_model/ConvLSTM_pytorch/convlstm.py:152\u001b[0m, in \u001b[0;36mConvLSTM.forward\u001b[0;34m(self, input_tensor, hidden_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# Since the init is done in forward. Can send image size here\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m layer_output_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    156\u001b[0m last_state_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/workspace/FLOOD_group2/models/ConvLSTM_model/ConvLSTM_pytorch/convlstm.py:189\u001b[0m, in \u001b[0;36mConvLSTM._init_hidden\u001b[0;34m(self, batch_size, image_size)\u001b[0m\n\u001b[1;32m    187\u001b[0m init_states \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m--> 189\u001b[0m     init_states\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m init_states\n",
      "File \u001b[0;32m/workspace/FLOOD_group2/models/ConvLSTM_model/ConvLSTM_pytorch/convlstm.py:58\u001b[0m, in \u001b[0;36mConvLSTMCell.init_hidden\u001b[0;34m(self, batch_size, image_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_hidden\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size, image_size):\n\u001b[1;32m     57\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m image_size\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m  \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     59\u001b[0m             torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim, height, width, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # Model training\n",
    "    train_loss = train_epoch_conv_lstm(model, train_loader, optimizer, device=device)\n",
    "\n",
    "    # Model validation\n",
    "    val_loss = evaluation_conv_lstm(model, val_loader, device=device)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "    \n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if epoch%100 == 0:\n",
    "        print(f\"Epoch: {epoch} \" +\n",
    "              f\"\\t Training loss: {train_loss: .2e} \" + \n",
    "              f\"\\t Validation loss: {val_loss: .2e} \" +\n",
    "              f\"\\t Best validation loss: {best_loss: .2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(best_model)\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0e35d-3866-4ff6-9ecb-d7b6ee0914a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, 'ConvLSTM (+ Augmentation) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b11304-05a6-4f6e-a8e4-bd567d050ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_animation(10, normalized_train_dataset, model, train_val,\n",
    "#                scaler_x, scaler_wd, scaler_q, device = device, save = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e8043-84d1-44a6-b80b-23d60e320cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sorted(dataset, train_val, scaler_x, scaler_wd, scaler_q, model, device):\n",
    "    '''\n",
    "    Function for plotting the DEMs variation sorted in increasing order \n",
    "    of average loss (of Water Depth and Discharge)\n",
    "\n",
    "    Input: dataset = tensor, normalized dataset\n",
    "           train_val_test : str, Identifier of dictionary. Expects: 'train_val', 'test1', 'test2', 'test3'.\n",
    "           scaler_x, scaler_wd, scaler_q = scalers for inputs (x) and targets (water depth and discharge), created \n",
    "                                            with the scaler function \n",
    "    Output: None (plot)\n",
    "    '''\n",
    "    \n",
    "    # get inputs and outputs\n",
    "    # 1st sample, 2nd input(0)/target(1), 3rd time step, 4th features, 5th/6th pixels\n",
    "    \n",
    "    # input = dataset[0][0]\n",
    "    # target = dataset[0][1]\n",
    "    \n",
    "    n_samples = len(dataset)\n",
    "    n_features = dataset[0][1].shape[1]\n",
    "    n_pixels = dataset[0][1].shape[-1]\n",
    "    time_steps = dataset[0][1].shape[0]\n",
    "    \n",
    "    # initialize inputs and outputs\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        inputs.append(dataset[i][0])\n",
    "        targets.append(dataset[i][1])\n",
    "\n",
    "    # initialize denormalization of dataset\n",
    "    elevations = np.zeros((n_samples, n_pixels, n_pixels))\n",
    "    water_depths = np.zeros((n_samples, time_steps, n_pixels, n_pixels))\n",
    "    discharges = np.zeros((n_samples, time_steps, n_pixels, n_pixels))\n",
    "    # print(discharges.shape)\n",
    "\n",
    "    # initialize losses\n",
    "    losses = torch.zeros((n_samples, n_features))\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        for t in range(time_steps):\n",
    "        # denormalize dataset\n",
    "            elevations[i], water_depths[i], discharges[i] = denormalize_dataset(inputs[i], targets[i], train_val, \n",
    "                                                            scaler_x, scaler_wd, scaler_q)\n",
    "        # make predictions\n",
    "        preds = obtain_predictions(model, inputs[i], device)\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            # compute MSE losses\n",
    "            losses[i, feature] = nn.MSELoss()(preds[:][feature], targets[i][:][feature]) # [:, feature]\n",
    "    \n",
    "    # print(water_depths.shape)\n",
    "    elevations_tensor = torch.tensor(elevations)\n",
    "\n",
    "    # print(len(water_depths))\n",
    "    # print(f'Shape wd: {water_depths.shape}')\n",
    "    # print(f'Shape q: {discharges.shape}')\n",
    "\n",
    "    # print(f'Size wd: {water_depths.size}')\n",
    "    # print(f'Size q: {discharges.size}')\n",
    "\n",
    "    # compute average loss for sorting dataset\n",
    "    \n",
    "    # loss with water depth, improve with normalized\n",
    "    avg_loss = torch.mean(losses, dim=1) \n",
    "    \n",
    "    # compute recall - improvement: add minimium threshold for recall (wd > 10 cm), need to denormalize targets and predictions\n",
    "    # ask scaler what 10 is and plot that scaler_wd.transform(0.10) - check\n",
    "    recall, _, _ = confusion_mat(dataset, model, device)\n",
    "\n",
    "    # sorting dataset\n",
    "    sorted_loss, sorted_indexes = torch.sort(avg_loss)\n",
    "    print(sorted_indexes)\n",
    "    # print(sorted_loss)\n",
    "    # print(np.shape(sorted_loss))\n",
    "    # sorted_indexes = torch.argsort(sorted_loss) #[index for index in sorted_loss]\n",
    "\n",
    "    elevation_sorted = elevations[sorted_indexes] #[elevations[i] for i in sorted_indexes]\n",
    "    # print(np.shape(elevation_sorted))\n",
    "    wd_sorted, q_sorted = water_depths[sorted_indexes], discharges[sorted_indexes] #[water_depths[i] for i in sorted_indexes], [discharges[i] for i in sorted_indexes]\n",
    "    sorted_recall = recall[sorted_indexes] #[recall[i] for i in sorted_indexes]\n",
    "    \n",
    "    # plot \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 10), sharex=True)\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "\n",
    "    # create second y-axis for discharge scale\n",
    "    ax1_2 = axes[1].twinx()\n",
    "    \n",
    "    axes[0].boxplot(sorted_indexes, elevation_sorted.all()) \n",
    "    axes[1].scatter(sorted_indexes, wd_sorted[:, 0, 0, 0], color='blue', label='water depth')\n",
    "    ax1_2.scatter(sorted_indexes, q_sorted[:, 0, 0, 0], color='red', label='discharge')\n",
    "    axes[2].scatter(sorted_indexes, sorted_recall, color='green', label='recall')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Sample ID')\n",
    "    \n",
    "    axes[0].set_ylabel('Normalized variation [-]')\n",
    "    axes[1].set_ylabel('Normalized Water Depth loss [-]')\n",
    "    ax1_2.set_ylabel('Normalized Discharge loss [-]')\n",
    "    axes[2].set_ylabel('Recall [-]')\n",
    "\n",
    "    axes[0].set_title('Normalized DEM variation [-]')\n",
    "    axes[1].set_title('Normalized MSE loss [-]')\n",
    "    axes[2].set_title('Recall [-]')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sorted(normalized_train_dataset, train_val, scaler_x, scaler_wd, scaler_q, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1a9e5-1438-4321-a5cf-e38a087bf93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_dataset[0][0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
