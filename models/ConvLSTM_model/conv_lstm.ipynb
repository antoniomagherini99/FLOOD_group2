{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Documents\\GitHub\\FLOOD_project\\models\n",
      "C:\\Users\\Admin\\Documents\\GitHub\\FLOOD_project\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%cd ..\n",
    "# move to the root directory of the git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\torch\\__init__.py:1504\u001b[0m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdlpack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dlpack, to_dlpack\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# Import experimental masked operations support. See\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# information.\u001b[39;00m\n\u001b[1;32m-> 1504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m masked\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;66;03m# Import removed ops with error message about removal\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linalg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m     matrix_rank,\n\u001b[0;32m   1509\u001b[0m     eig,\n\u001b[0;32m   1510\u001b[0m     solve,\n\u001b[0;32m   1511\u001b[0m     lstsq,\n\u001b[0;32m   1512\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\torch\\masked\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaskedtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_masked_tensor, MaskedTensor\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaskedtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcreation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m as_masked_tensor, masked_tensor\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     _canonical_dim,\n\u001b[0;32m      5\u001b[0m     _generate_docstring,\n\u001b[0;32m      6\u001b[0m     _reduction_identity,\n\u001b[0;32m      7\u001b[0m     _where,\n\u001b[0;32m      8\u001b[0m     _input_mask,\n\u001b[0;32m      9\u001b[0m     _output_mask,\n\u001b[0;32m     10\u001b[0m     _combine_input_and_mask,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28msum\u001b[39m,\n\u001b[0;32m     12\u001b[0m     prod,\n\u001b[0;32m     13\u001b[0m     cumsum,\n\u001b[0;32m     14\u001b[0m     cumprod,\n\u001b[0;32m     15\u001b[0m     amax,\n\u001b[0;32m     16\u001b[0m     amin,\n\u001b[0;32m     17\u001b[0m     argmax,\n\u001b[0;32m     18\u001b[0m     argmin,\n\u001b[0;32m     19\u001b[0m     mean,\n\u001b[0;32m     20\u001b[0m     median,\n\u001b[0;32m     21\u001b[0m     logsumexp,\n\u001b[0;32m     22\u001b[0m     logaddexp,\n\u001b[0;32m     23\u001b[0m     norm,\n\u001b[0;32m     24\u001b[0m     var,\n\u001b[0;32m     25\u001b[0m     std,\n\u001b[0;32m     26\u001b[0m     softmax,\n\u001b[0;32m     27\u001b[0m     log_softmax,\n\u001b[0;32m     28\u001b[0m     softmin,\n\u001b[0;32m     29\u001b[0m     normalize,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_masked_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_masked_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasked_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaskedTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\torch\\masked\\_ops.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmasked\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m as_masked_tensor, is_masked_tensor, MaskedTensor\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _docs\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corresponding_real_dtype\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sym_float\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\torch\\_prims_common\\__init__.py:23\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cmp_to_key, reduce\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     Any,\n\u001b[0;32m     12\u001b[0m     Callable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     Union,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sym_float, sym_int, sym_max\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\sympy\\__init__.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m mpmath\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelease\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lazy_function\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m __version__:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menable_warnings\u001b[39m():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\sympy\\core\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Core module. Provides the basic operations needed in sympy.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msympify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sympify, SympifyError\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cacheit\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massumptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assumptions, check_assumptions, failing_assumptions, common_assumptions\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\sympy\\core\\sympify.py:634\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr\n\u001b[0;32m    633\u001b[0m \u001b[38;5;66;03m# Avoid circular import\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Basic\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\sympy\\core\\basic.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mapping\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chain, zip_longest\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massumptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _prepare_class_assumptions\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cacheit\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ordering_of_classes\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\sympy\\core\\assumptions.py:214\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis module contains the machinery handling assumptions.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mDo also consider the guide :ref:`assumptions-guide`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sympy_deprecation_warning\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfacts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FactRules, FactKB\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msympify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sympify\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _assumptions_shuffle \u001b[38;5;28;01mas\u001b[39;00m shuffle\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:982\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:925\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1423\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1395\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1522\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:142\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch.nn as nn\n",
    "\n",
    "# Enable interactive widgets in Jupyter Notebook\n",
    "%matplotlib widget\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from models.ConvLSTM_model.ConvLSTM_pytorch.multistep_convlstm import MultiStepConvLSTM\n",
    "from models.ConvLSTM_model.ConvLSTM_pytorch.convlstm import ConvLSTM\n",
    "from models.ConvLSTM_model.train_eval import train_epoch_conv_lstm, evaluation_conv_lstm\n",
    "from pre_processing.encode_decode_csv import decode_from_csv\n",
    "from pre_processing.normalization import * \n",
    "from post_processing.cool_animation import plot_animation\n",
    "from post_processing.plots import plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save path\n",
    "save_path = 'models/ConvLSTM_model/multiconv_lstm_10hid_4lay_5ker.pth'\n",
    "\n",
    "# if using a \"simple\" ConvLSTM model uncomment the following line and comment the one above\n",
    "# save_path = 'models/ConvLSTM_model/conv_lstm_16batch_32hidden_48layers.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines create variables to more easily specify what we use the model for (i.e., train and validate, test with dataset 1 and so on) in the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = 'train_val'\n",
    "test1 = 'test1'\n",
    "test2 = 'test2'\n",
    "test3 = 'test3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note**: \n",
    "\\\n",
    "inputs and targets in <code>train_val</code> are not stored in the original manner. \n",
    "\n",
    "index 0 = 1, index 1 = 10, index 11 = 2, index 12 = 20 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation dataset\n",
    "train_dataset = decode_from_csv(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first position is the samples, second decides between inputs or targets, notice new shape of inputs\n",
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# From FAT application\n",
    "fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "# specify which entry of the dataset to plot\n",
    "numb = 0\n",
    "inputs = train_dataset[numb][0][0]\n",
    "\n",
    "axs[0].imshow(inputs[0].cpu(), cmap='terrain', origin='lower')\n",
    "axs[0].set_title('DEM')\n",
    "\n",
    "axs[1].imshow(inputs[1].cpu(), cmap='RdBu', origin='lower')\n",
    "axs[1].set_title('Slope X')\n",
    "\n",
    "axs[2].imshow(inputs[2].cpu(), cmap='RdBu', origin='lower')\n",
    "axs[2].set_title('Slope Y')\n",
    "\n",
    "non_zero_indices = torch.nonzero(inputs[3].cpu())\n",
    "non_zero_row, non_zero_col = non_zero_indices[0][0].item(), non_zero_indices[0][1].item()\n",
    "axs[3].imshow(inputs[3].cpu(), cmap='binary', origin='lower')\n",
    "axs[3].set_title('Breach Location')\n",
    "axs[3].scatter(non_zero_col, non_zero_row, color='k', marker='x', s=100,\n",
    "                clip_on = False, clip_box = plt.gca().transData)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "train_percnt = 0.8\n",
    "train_size = int(train_percnt * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs using training dataset\n",
    "scaler_x, scaler_y = scaler(train_set)\n",
    "\n",
    "normalized_train_dataset = normalize_dataset(train_set, scaler_x, scaler_y, train_val)\n",
    "normalized_val_dataset = normalize_dataset(val_set, scaler_x, scaler_y, train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = MultiStepConvLSTM(input_dim = normalized_train_dataset[0][0].shape[1],\n",
    "                          output_dim = normalized_train_dataset[0][1].shape[1], \n",
    "                          hidden_dim = 10, kernel_size = (5, 5), num_layers = 4,\n",
    "                          batch_first=True, bias=True, return_all_layers = False).to(device)\n",
    "\n",
    "# if using a \"simple\" ConvLSTM model uncomment the following line and comment the ones above\n",
    "# model = ConvLSTM(input_dim = normalized_train_dataset[0][0].shape[1], output_dim = normalized_train_dataset[0][1].shape[1], hidden_dim = 32, kernel_size = (3, 3),\n",
    "#                  num_layers = 48, batch_first=True, bias=True, return_all_layers = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "# why are there 4 less cells states in the Conv2D (?) - possible question for the presentation, from Roberto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {num_parameters}\")\n",
    "model_size_MB = num_parameters * 4 / (1024 ** 2)  # Assuming float32 precision\n",
    "print(f\"Model size: {model_size_MB:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.0008\n",
    "batch_size = 8 # Only have 64 and 16 samples for training and validation, I think should be kept small, having issues where this only works if set to 1\n",
    "# num_epochs = 3000\n",
    "num_epochs = 500\n",
    "\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "count = 0\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # print(f\"epoch number: {epoch}\")\n",
    "    # Model training\n",
    "    train_loss = train_epoch_conv_lstm(model, train_loader, optimizer, device)\n",
    "\n",
    "    # Model validation\n",
    "    val_loss = evaluation_conv_lstm(model, val_loader, device)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "    \n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        count = 0\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    count += 1\n",
    "    if epoch%10 == 0:\n",
    "        print(f\"Epoch: {epoch} \" +\n",
    "              f\"\\t Training loss: {train_loss: .2e} \" + \n",
    "              f\"\\t Validation loss: {val_loss: .2e} \" +\n",
    "              f\"\\t Best validation loss: {best_loss: .2e}\")\n",
    "    stop_count = 100\n",
    "    if count > stop_count:\n",
    "        print(f\"Loss Stagnated for {stop_count} epochs, early stopping initiated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(best_model)\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, 'MultiStepConvLSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_dataset[0][1][-1,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(1, train_set, model, train_val,\n",
    "               scaler_x, scaler_y, device = device, save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
