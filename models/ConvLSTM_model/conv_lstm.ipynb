{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%cd ..\n",
    "# move to the root directory of the git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch.nn as nn\n",
    "\n",
    "# Enable interactive widgets in Jupyter Notebook\n",
    "%matplotlib widget\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from models.ConvLSTM_model.ConvLSTM_pytorch.convlstm import ConvLSTM\n",
    "from pre_processing.encode_decode_csv import decode_from_csv\n",
    "from pre_processing.normalization import * \n",
    "from post_processing.cool_animation import plot_animation\n",
    "from post_processing.plots import plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(convlstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines create variables to more easily specify what we use the model for (i.e., train and validate, test with dataset 1 and so on) in the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = 'train_val'\n",
    "test1 = 'test1'\n",
    "test2 = 'test2'\n",
    "test3 = 'test3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note**: \n",
    "\\\n",
    "inputs and targets in <code>train_val</code> are stored not in the original manner. \n",
    "\n",
    "index 0 = 1, index 1 = 10, index 11 = 2, index 12 = 20 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation dataset\n",
    "train_dataset = decode_from_csv(train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first position is the samples, second decides between inputs or targets, notice new shape of inputs\n",
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset 1\n",
    "test_dataset1 = decode_from_csv(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset 2\n",
    "test_dataset2 = decode_from_csv(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset 3\n",
    "test_dataset3 = decode_from_csv(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From FAT application\n",
    "fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "# specify which entry of the dataset to plot\n",
    "numb = 0\n",
    "\n",
    "axs[0].imshow(train_dataset[numb][0][0][0].cpu(), cmap='terrain', origin='lower')\n",
    "axs[0].set_title('DEM')\n",
    "\n",
    "axs[1].imshow(train_dataset[numb][0][0][1].cpu(), cmap='RdBu', origin='lower')\n",
    "axs[1].set_title('Slope X')\n",
    "\n",
    "axs[2].imshow(train_dataset[numb][0][0][2].cpu(), cmap='RdBu', origin='lower')\n",
    "axs[2].set_title('Slope Y')\n",
    "\n",
    "axs[3].imshow(train_dataset[numb][0][0][3].cpu(), cmap='Set1', origin='lower')\n",
    "axs[3].set_title('Water Depth time step 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animation(2, train_dataset, train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs using training dataset\n",
    "scaler_x, scaler_y = scaler(train_dataset)\n",
    "\n",
    "normalized_train_dataset = normalize_dataset(train_dataset, scaler_x, scaler_y)\n",
    "normalized_test_dataset = normalize_dataset(test_dataset1, scaler_x, scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "train_percnt = 0.8\n",
    "train_size = int(train_percnt * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(normalized_train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, check, not sure num_layers is the expected amount of outputs\n",
    "model = ConvLSTM(input_dim = train_dataset[0][0].shape[1], hidden_dim = 2, kernel_size = (3, 3), num_layers = 1,\n",
    "         batch_first=True, bias=False, return_all_layers = True).to(device)\n",
    "# return all layers has to be true to obtain all the outputs I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train() # specifies that the model is in training mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        # Model prediction\n",
    "        preds, _ = model(x)\n",
    "        list_preds = torch.cat(preds, dim=0)\n",
    "\n",
    "        # MSE loss function\n",
    "        loss = nn.MSELoss()(list_preds, y)\n",
    "        \n",
    "        losses.append(loss.cpu().detach())\n",
    "        \n",
    "        # Backpropagate and update weights\n",
    "        loss.backward()   # compute the gradients using backpropagation\n",
    "        optimizer.step()  # update the weights with the optimizer\n",
    "        optimizer.zero_grad(set_to_none=True)   # reset the computed gradients\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval() # specifies that the model is in evaluation mode \n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        # Model prediction\n",
    "        preds, _ = model(x)\n",
    "        list_preds = torch.cat(preds, dim=0)\n",
    "\n",
    "        # MSE loss function\n",
    "        loss = nn.MSELoss()(list_preds, y)\n",
    "        \n",
    "        losses.append(loss.cpu().detach())\n",
    "        \n",
    "        # Backpropagate and update weights\n",
    "        loss.backward()   # compute the gradients using backpropagation\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 16 # Only have 64 and 16 samples for training and validation, I think should be kept small\n",
    "num_epochs = 400\n",
    "\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # Model training\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device=device)\n",
    "\n",
    "    # Model validation\n",
    "    val_loss = evaluation(model, val_loader, device=device)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "    \n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(\"epoch:\",epoch, \"\\t training loss:\", np.round(train_loss,4),\n",
    "                            \"\\t validation loss:\", np.round(val_loss,4))\n",
    "        \n",
    "model = copy.deepcopy(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, 'ConvLSTM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
