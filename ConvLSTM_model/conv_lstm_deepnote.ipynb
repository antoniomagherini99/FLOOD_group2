{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"6745e274f5da440d8f75866d3a55b9d6","deepnote_cell_type":"markdown"},"source":["# ConvLSTM model"]},{"cell_type":"markdown","metadata":{"cell_id":"6e15ddc3ae0f40bda0d72baaa696cba1","deepnote_cell_type":"markdown"},"source":["Import libraries and modules."]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"317a1931967a493a820fae093f628cfa","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":777,"execution_start":1703258901929,"source_hash":"26b410bf"},"outputs":[],"source":["import torch\n","import os\n","# import imageio\n","\n","import numpy as np\n","import pandas as pd\n","# import matplotlib.pyplot as plt\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","\n","# from numba import jit, prange\n","\n","# from PIL import Image\n","# from sklearn.preprocessing import MinMaxScaler\n","# from torchsummary import summary\n","# from torch.utils.data import DataLoader\n","# from matplotlib.colors import TwoSlopeNorm\n","\n","from load_datasets import *\n","from ConvLSTM_pytorch import *"]},{"cell_type":"markdown","metadata":{"cell_id":"3b8c87dfefcc4dcb9099e17e1193a568","deepnote_cell_type":"markdown"},"source":["Check if GPU is available."]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"388661982cff45b2bff8175da7fa4243","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":8,"execution_start":1703258909998,"source_hash":"407af93f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')"]},{"cell_type":"markdown","metadata":{"cell_id":"3a0f5b87a9e341119392b2536f71e1c7","deepnote_cell_type":"markdown"},"source":["The following paths access the main folder (i.e., _dataset_train_val_, _dataset1_ and so on). The path of the specific type of data (_DEM_, _VX_ and so on) is to be specified after."]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"26b1b73dd262445c884866e82c26f594","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":40,"execution_start":1703259005592,"source_hash":"5f5b5265"},"outputs":[],"source":["path_train = f'../dataset_train_val/' \n","path_test1 = f'../dataset1/'\n","path_test2 = f'../dataset2/'\n","path_test3 = f'../dataset3/'"]},{"cell_type":"markdown","metadata":{"cell_id":"4c107907144b43e5863002290b74aa57","deepnote_cell_type":"markdown"},"source":["The following lines create variables to more easily specify what we use the model for (i.e., train and validate, test with dataset 1 and so on) in the following functions."]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"6b7157af37c844b28bc5bf399b6be62b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":8,"execution_start":1703258911722,"source_hash":"2f7e85f"},"outputs":[],"source":["train_val = 'train_val'\n","test1 = 'test1'\n","test2 = 'test2'\n","test3 = 'test3'"]},{"cell_type":"markdown","metadata":{"cell_id":"9bdaef464b9b47cba6bbee3de33e2d32","deepnote_cell_type":"markdown"},"source":["Load data."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e0c9b1b5f426489bb5e1009b5d3b2345","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":86552,"execution_start":1703262231238,"source_hash":"4ff12bd1"},"outputs":[],"source":["# inputs, targets = load_all_boys('train_val')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def encode_into_csv(inputs, targets, train_val_test):\n","    \"\"\"\n","    Due to the long run time of computing all inputs and targets, these will be encoded into a csv file\n","    to reduce the computatio duration\n","\n","    Input:\n","    inputs: torch.tensor of shape: samples x 3 x 64 x 64 which represents the inputs of the network\n","    targets: torch.tensor of shape: samples x time steps x 64 x 64 which represents the targets of the network\n","    train_val_test: str, differentiate between csv files\n","\n","    Outputs:\n","    None: But a csv file is create with a predetermined name\n","    \"\"\"\n","    # Flatten the tensors and concatenate them along the specified dimension\n","    flattened_tensor1 = torch.flatten(inputs, start_dim=0)\n","    flattened_tensor2 = torch.flatten(targets, start_dim=0)\n","\n","    # Convert the tensor to a pandas DataFrame\n","    df_inputs = pd.DataFrame(flattened_tensor1.numpy())\n","    df_targets = pd.DataFrame(flattened_tensor2.numpy())\n","\n","    # Save the DataFrame to a CSV file\n","    df_inputs.to_csv(train_val_test + '_in.csv', index=False)\n","\n","    # if train_val_test = 'train_val' targets file is too big to be loaded in GitHub\n","    # and it needs to be split into 4 different .csv files\n","    # n_tot = 63569920 total number of rows of targets (80x2x97x64x64)\n","    # n = n_tot/4 to split in 4 separate files\n","    n_tot = int(targets.size(0) * targets.size(1) * targets.size(2) * targets.size(3) * targets.size(4))\n","    n = int(n_tot / 4)\n","\n","    if train_val_test == 'train_val':\n","        df_targets[:n].to_csv(train_val_test + '_tar1.csv', index=False)\n","        df_targets[n:2*n].to_csv(train_val_test + '_tar2.csv', index=False)\n","        df_targets[2*n:3*n].to_csv(train_val_test + '_tar3.csv', index=False)\n","        df_targets[3*n:].to_csv(train_val_test + '_tar4.csv', index=False)\n","    else: \n","        df_targets.to_csv(train_val_test + '_tar.csv', index=False)\n","    return df_inputs, df_targets"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def decode_from_csv(train_val_test):\n","    \"\"\"\n","    Due to the long run time of computing all inputs and targets, a csv file will be opened\n","    at the start of every notebook which represents the inputs and targets for a certain dataset\n","\n","    Input:\n","    train_val_test: str, identifies which dataset is being retrieved\n","\n","    Output:\n","    inputs: torch.Tensor which contains DEM, slope x and y for all files in a dataset\n","            Shape is samples x 3 x 64 x 64\n","    targets: torch.Tensor which contains water depth and discharge for all files in a dataset.\n","            Shape is samples x time steps x 2 x 64 x 64\n","    \"\"\"\n","    df_inputs = pd.read_csv(train_val_test + '_in.csv')\n","    \n","    # if train_val_test = 'train_val' targets file is too big to be loaded in GitHub\n","    # and it needs to be split into 4 different .csv files\n","    if train_val_test == 'train_val':\n","        df_targets1 = pd.read_csv(train_val_test + '_tar1.csv')\n","        df_targets2 = pd.read_csv(train_val_test + '_tar2.csv')\n","        df_targets3 = pd.read_csv(train_val_test + '_tar3.csv')\n","        df_targets4 = pd.read_csv(train_val_test + '_tar4.csv')\n","\n","        df_targets = pd.concat([df_targets1, df_targets2, \n","                                df_targets3, df_targets4], axis=0) \n","    else:\n","        df_targets = pd.read_csv(train_val_test + '_tar.csv')\n","\n","    # Convert the DataFrame to a PyTorch tensor\n","    restored_inputs = torch.tensor(df_inputs.values)\n","    restored_targets = torch.tensor(df_targets.values)\n","\n","    # Determine the original shapes of the tensors\n","    if 'train_val':\n","        samples = 80\n","    elif 'test1':\n","        samples = 21\n","    elif 'test2':\n","        samples = 20\n","    else:\n","        samples = 10\n","\n","    shape_tensor1 = (samples, 3, 64, 64)\n","    shape_tensor2 = (samples, 97, 2, 64, 64)\n","\n","    # Split the restored tensor into two tensors based on the original shapes\n","    inputs = torch.reshape(restored_inputs, shape_tensor1)\n","    targets = torch.reshape(restored_targets, shape_tensor2)\n","\n","    # Print the shapes of the restored tensors\n","    print(\"Restored inputs Shape:\", inputs.shape)\n","    print(\"Restored targets Shape:\", targets.shape)\n","    return inputs, targets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# inps, targs = encode_into_csv(inputs, targets, train_val)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# inputs, targets = decode_from_csv(train_val)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# not needed anymore?\n","\n","# count = 0\n","# dir_path = path_train + 'DEM/' # Arbitrary choice as DEM, vx, vy and WD all have the same number of samples\n","# for path in os.listdir(dir_path):\n","#     if os.path.isfile(os.path.join(dir_path, path)):\n","#         count += 1\n","# inputs = torch.zeros((count, 3, 64, 64))\n","# targets = torch.zeros((count, 97, 2, 64, 64))\n","# print(count)"]},{"cell_type":"markdown","metadata":{},"source":["Test dataset 1."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def process_elevation_data(file_id, train_val_test='train_val'):\n","    \"\"\"\n","    Processes elevation data from a DEM file.\n","\n","    Input:\n","    file_id (str): Identifier of the DEM file to be processed.\n","    train_val_test: key for specifying what we are using the model for\n","                   'train_val' = train and validate the model\n","                   'test1' = test the model with dataset 1\n","                   'test2' = test the model with dataset 2\n","                   'test3' = test the model with dataset 3\n","\n","    Output:\n","    torch.Tensor: A tensor combining the original elevation data and its slope in x and y directions.\n","    \"\"\"\n","    # specify what we use the model for -- so far works for only one specified input (i.e., file_id), \n","    # will need to be improved to work for all inputs regardless of the number of the file\n","    if train_val_test == 'train_val':\n","        file_path = path_train + f'DEM/DEM_{file_id}.txt'\n","    elif train_val_test == 'test1':\n","        file_path = path_test1 + f'DEM/DEM_{500 + file_id}.txt'\n","    elif train_val_test == 'test2':\n","        file_path = path_test2 + f'DEM/DEM_{10000 + file_id}.txt'\n","    elif train_val_test == 'test3':\n","        file_path = path_test3 + f'DEM/DEM_{15001 + file_id}.txt'\n","\n","    # # Construct the file path from the given file identifier\n","    # file_path = f'DEM_{file_id}.txt'\n","\n","    # Load the elevation data from the file\n","    elevation_data = np.loadtxt(file_path)\n","\n","    # Reshape the elevation data into a 64x64 grid\n","    elevation_grid = elevation_data[:, 2].reshape(64, 64)\n","\n","    # Convert the elevation grid to a PyTorch tensor\n","    elevation_tensor = torch.tensor(elevation_grid)\n","\n","    # Compute the slope in the x and y directions\n","    slope_x, slope_y = torch.gradient(elevation_tensor)\n","\n","    # Combine the elevation tensor with the slope tensors\n","    elevation_slope_tensor = torch.stack((elevation_tensor, slope_x, slope_y), dim=0)\n","\n","    return elevation_slope_tensor\n","\n","# ------------- #\n","\n","def process_water_depth(file_id, train_val_test='train_val', time_step=0):\n","    \"\"\"\n","    Processes water depth data from a specific time step in a file.\n","\n","    Args:\n","    file_id (str): Identifier of the water depth file to be processed.\n","    train_val_test: key for specifying what we are using the model for\n","                   'train_val' = train and validate the model\n","                   'test1' = test the model with dataset 1\n","                   'test2' = test the model with dataset 2\n","                   'test3' = test the model with dataset 3\n","    time_step (int): Time step to extract from the file. Default is the first time step. Default is the first time step.\n","\n","    Returns:\n","    torch.Tensor or None: A 64x64 tensor representing water depth at the given time step, or None if the data is invalid.\n","    \"\"\"\n","    # specify what we use the model for -- so far works for only one specified input (i.e., file_id), \n","    # will need to be improved to work for all inputs regardless of the number of the file\n","    if train_val_test == 'train_val':\n","        file_path = path_train + f'WD/WD_{file_id}.txt'\n","    elif train_val_test == 'test1':\n","        file_path = path_test1 + f'WD/WD_{500 + file_id}.txt'\n","    elif train_val_test == 'test2':\n","        file_path = path_test2 + f'WD/WD_{10000 + file_id}.txt'\n","    elif train_val_test == 'test3':\n","        file_path = path_test3 + f'WD/WD_{15001 + file_id}.txt'\n","\n","    # Read the file\n","    with open(file_path, 'r') as file:\n","        lines = file.readlines()\n","\n","    try:\n","        # Extract the specified row and convert string elements to floats\n","        selected_row = lines[time_step].split()\n","        depth_values = [float(val) for val in selected_row]\n","\n","        # Validate and reshape the data into a 64x64 tensor\n","        if len(depth_values) == 64 * 64:\n","            depth_tensor = torch.tensor(depth_values).view(64, 64)\n","            return depth_tensor\n","        else:\n","            raise ValueError(f\"The number of elements in {file_path} at time step {time_step} doesn't match a 64x64 matrix.\")\n","    except IndexError:\n","        raise IndexError(f\"Time step {time_step} is out of range for the file {file_path}.\")\n","\n","# ------------- #\n","\n","def process_velocities(file_id, train_val_test='train_val', time_step=0):\n","    \"\"\"\n","    Processes elevation data from a DEM file.\n","\n","    Input:\n","    file_id (str): Identifier of the DEM file to be processed.\n","    train_val_test: key for specifying what we are using the model for\n","                   'train_val' = train and validate the model\n","                   'test1' = test the model with dataset 1\n","                   'test2' = test the model with dataset 2\n","                   'test3' = test the model with dataset 3\n","    time_step (int): Time step to extract from the file. Default is the first time step. Default is the first time step.\n","\n","    Output:\n","    torch.Tensor: A tensor combining the original elevation data and its slope in x and y directions.\n","    \"\"\"\n","    # specify what we use the model for -- so far works for only one specified input (i.e., file_id), \n","    # will need to be improved to work for all inputs regardless of the number of the file\n","    if train_val_test == 'train_val':\n","        file_path_x = path_train + f'VX/VX_{file_id}.txt'\n","        file_path_y = path_train + f'VY/VY_{file_id}.txt'\n","    \n","    elif train_val_test == 'test1':\n","        file_path_x = path_test1 + f'VX/VX_{500 + file_id}.txt'\n","        file_path_y = path_test1 + f'VY/VY_{500 + file_id}.txt'\n","    \n","    elif train_val_test == 'test2':\n","        file_path_x = path_test2 + f'VX/VX_{10000 + file_id}.txt'\n","        file_path_y = path_test2 + f'VY/VY_{10000 + file_id}.txt'\n","    \n","    elif train_val_test == 'test3':\n","        file_path_x = path_test3 + f'VX/VX_{15001 + file_id}.txt'\n","        file_path_y = path_test3 + f'VY/VY_{15001 + file_id}.txt'\n","\n","    # Load the elevation data from the file\n","    vx, vy = np.loadtxt(file_path_x), np.loadtxt(file_path_y)\n","\n","    # Read the file\n","    with open(file_path_x, 'r') as file:\n","        lines_x = file.readlines()\n","    \n","    with open(file_path_y, 'r') as file:\n","        lines_y = file.readlines()\n","\n","    try:\n","        # Extract the specified row and convert string elements to floats\n","        selected_row_x = lines_x[time_step].split()\n","        selected_row_y = lines_y[time_step].split()\n","        \n","        vel_x = [float(val) for val in selected_row_x]\n","        vel_y = [float(val) for val in selected_row_y]\n","\n","        # Validate and reshape the data into a 64x64 tensor\n","        if (len(vel_x) == 64 * 64) and (len(vel_y) == 64 * 64):\n","            vel_x = torch.tensor(vel_x).view(64, 64)\n","            vel_y = torch.tensor(vel_y).view(64, 64)\n","            return vel_x, vel_y\n","        else:\n","            raise ValueError(f\"The number of elements in {file_path_x} or {file_path_y} at time step {time_step} doesn't match a 64x64 matrix.\")\n","    except IndexError:\n","        raise IndexError(f\"Time step {time_step} is out of range for the file {file_path_x} or {file_path_y}.\")\n","\n","# ------------- #\n","\n","def compute_targets(file_id, train_val_test='train_val', time_step = 0):\n","    \"\"\"\n","    Use process_velocities and process_water_depth to compute discharge\n","\n","    Input:\n","    file_id (str): Identifier of the DEM file to be processed.\n","    train_val_test: key for specifying what we are using the model for\n","                   'train_val' = train and validate the model\n","                   'test1' = test the model with dataset 1\n","                   'test2' = test the model with dataset 2\n","                   'test3' = test the model with dataset 3\n","\n","    Output:\n","    targets: A torch.tensor which is 2 x 64 x 64. Both targets are water depth and discharge respectively\n","    \"\"\"\n","    water_depth = process_water_depth(file_id, train_val_test, time_step)\n","    vx, vy = process_velocities(file_id, train_val_test, time_step)\n","\n","    magnitude = torch.sqrt(vx**2 + vy**2)\n","    discharge = water_depth * magnitude # per meter width\n","\n","    targets = torch.stack((water_depth, discharge), dim=0)\n","    return targets\n","\n","# ------------- #\n","\n","# @jit(parallel=True)\n","def load_all_boys(train_val_test='train_val', time=97):\n","    '''\n","    Load all \"file_id\" and \"time_step\" for chosen dataset\n","\n","    Input: \n","    train_val_test = key for choosing dataset  \n","         = 'train_val', 'test1', 'test2', 'test3'\n","    time = time step of simulation # 97 is hardcoded !\n","\n","    Output:\n","    inputs: torch.Tensor which contains DEM, slope x and y for all files in a dataset\n","            Shape is samples x 3 x 64 x 64\n","    targets: torch.Tensor which contains water depth and discharge for all files in a dataset.\n","             Shape is samples x time steps x 2 x 64 x 64\n","    '''\n","    if train_val_test == 'train_val':\n","        file_path = path_train\n","    elif train_val_test == 'test1':\n","        file_path = path_test1\n","    elif train_val_test == 'test2':\n","        file_path = path_test2\n","    elif train_val_test == 'test3':\n","        file_path = path_test3\n","    \n","    # if train_val_test == 'train_val' or train_val_test == 'test3': \n","    count = 0\n","    dir_path = file_path + 'DEM' # Arbitrary choice as DEM, vx, vy and WD all have the same number of samples\n","    for path in os.listdir(dir_path):\n","        if os.path.isfile(os.path.join(dir_path, path)):\n","            count += 1\n","    inputs = torch.zeros((count, 3, 64, 64))\n","    targets = torch.zeros((count, time, 2, 64, 64))\n","\n","    for i in range(count):\n","        print(i)\n","        inputs[i] = process_elevation_data(i + 1, train_val_test)\n","        for t in range(time):\n","            targets[i, t] = compute_targets(i + 1, train_val_test, time_step = t)\n","    return inputs, targets"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n"]},{"ename":"FileNotFoundError","evalue":"../dataset1/DEM/DEM_520.txt not found.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inps1, targs1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_boys\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[11], line 221\u001b[0m, in \u001b[0;36mload_all_boys\u001b[1;34m(train_val_test, time)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(count):\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m--> 221\u001b[0m     inputs[i] \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_elevation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_val_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(time):\n\u001b[0;32m    223\u001b[0m         targets[i, t] \u001b[38;5;241m=\u001b[39m compute_targets(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, train_val_test, time_step \u001b[38;5;241m=\u001b[39m t)\n","Cell \u001b[1;32mIn[11], line 31\u001b[0m, in \u001b[0;36mprocess_elevation_data\u001b[1;34m(file_id, train_val_test)\u001b[0m\n\u001b[0;32m     25\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m path_test3 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEM/DEM_1500\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# # Construct the file path from the given file identifier\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# file_path = f'DEM_{file_id}.txt'\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Load the elevation data from the file\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m elevation_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Reshape the elevation data into a 64x64 grid\u001b[39;00m\n\u001b[0;32m     34\u001b[0m elevation_grid \u001b[38;5;241m=\u001b[39m elevation_data[:, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\npyio.py:1338\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1336\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1338\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\npyio.py:975\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    973\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 975\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mFileNotFoundError\u001b[0m: ../dataset1/DEM/DEM_520.txt not found."]}],"source":["inps1, targs1 = load_all_boys('test1')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'inps1' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inps1, targs1 \u001b[38;5;241m=\u001b[39m encode_into_csv(\u001b[43minps1\u001b[49m, targs1, test1)\n","\u001b[1;31mNameError\u001b[0m: name 'inps1' is not defined"]}],"source":["inps1, targs1 = encode_into_csv(inps1, targs1, test1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inps1, targs1 = decode_from_csv(test1)"]},{"cell_type":"markdown","metadata":{},"source":["Test dataset 2."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inps2, targs2 = load_all_boys(test2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inps2, targs2 = encode_into_csv(inps2, targs2, test2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inps1, targs1 = decode_from_csv(test2)"]},{"cell_type":"markdown","metadata":{},"source":["Test dataset 3."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# doesn't work, dataset 3 contains 128x128 grid data - need to change function\n","# inps3, targs3 = load_all_boys(test3)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# inps3, targs3 = encode_into_csv(inps3, targs3, test3)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# inps3, targs3 = decode_from_csv(test3)"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"4b953ff8b2a04095993ca3c1d8d4427b","deepnote_persisted_session":{"createdAt":"2023-12-22T15:19:57.116Z"},"kernelspec":{"display_name":"dsaie","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}
