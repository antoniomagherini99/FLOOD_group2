{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66242d2f",
   "metadata": {},
   "source": [
    "# CNN model notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db14a1",
   "metadata": {},
   "source": [
    "Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5de71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DJNaj\\Documents\\CiTG\\MSc\\Year II\\Q2\\CEGM2003 - Data Science and Artificial Intelligence for Engineers\\Unit 3 - Project\\FLOOD_group2\n"
     ]
    }
   ],
   "source": [
    "# move to the root directory of the git\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df834647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import imageio.v2 as imageio \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import random\n",
    "from IPython.display import Image, display\n",
    "import time\n",
    "\n",
    "from pre_processing.encode_decode_csv import decode_from_csv\n",
    "from pre_processing.process_data import *\n",
    "from post_processing.plots import create_combined_gif\n",
    "from models.CNN_model.CNN_functions import *\n",
    "from models.CNN_model.CNN_classes import *\n",
    "from pre_processing.augmentation import *\n",
    "from pre_processing.normalization import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30bcd99-9c3d-4675-bd96-7d39ec739541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ccc13",
   "metadata": {},
   "source": [
    "# Creating Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc77c2",
   "metadata": {},
   "source": [
    "First the training dataset is created based upon a research paper by Bentivoglio et al (2023). For training and validation purposes a dataset of 80 different generated digital elevation models are used to reflect different plausible topographies. The velocity in both x and y direction (VX and VY) as well as the water depth (WD) are known, based on numerical computations utilizing Delft3D.\n",
    "\n",
    "The code cell below uses two custom functions for the elevation data processing and water depth processing. \n",
    "\n",
    "<b>Elevation Data Processing</b>\n",
    "\n",
    "The process_elevation_data function takes a DEM (Digital Elevation Model) file identifier (file_id) and a dataset identifier (dataset_id) as inputs. It reads the corresponding DEM file, extracts elevation data, and calculates the slope in the x and y directions. The result is a torch tensor combining the original elevation data with its slope information.\n",
    "\n",
    "<b>Water Depth Processing</b>\n",
    "\n",
    "The process_water_depth function processes water depth data from a specific time step in a file. It requires a water depth file identifier (file_id), a dataset identifier (dataset_id), and an optional time step parameter (time_step, default is 0). The function reads the specified file, extracts the data for the given time step, and returns a 64x64 torch tensor representing water depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa540ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored inputs Shape: torch.Size([80, 1, 4, 64, 64])\n",
      "Restored targets Shape: torch.Size([80, 48, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "def remove_discharge(train_val_test):\n",
    "    '''\n",
    "    Print is located within decode_csv\n",
    "    '''\n",
    "    dataset = decode_from_csv(train_val_test)\n",
    "    inputs = dataset[:][0]\n",
    "    targets = dataset[:][1]\n",
    "    targets = targets[:,:, 0].unsqueeze (2) # remove discharge\n",
    "    new_dataset = TensorDataset(inputs.float(), targets.float())\n",
    "    return new_dataset\n",
    "train_dataset = remove_discharge('train_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b9533",
   "metadata": {},
   "source": [
    "# Creating the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ef268",
   "metadata": {},
   "source": [
    "Now the test dataset is created based on dataset1 from Bentivoglio et al (2023). This dataset features 20 DEMs over a squared domain of 64x64 grids of length 100 m and a simulation time of 48 h. A fixed breach location is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc4f40b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored inputs Shape: torch.Size([20, 1, 4, 64, 64])\n",
      "Restored targets Shape: torch.Size([20, 48, 2, 64, 64])\n",
      "Restored inputs Shape: torch.Size([21, 1, 4, 64, 64])\n",
      "Restored targets Shape: torch.Size([21, 48, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test_dataset1 = remove_discharge('test1')\n",
    "test_dataset2 = remove_discharge('test2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e01e5",
   "metadata": {},
   "source": [
    "# Aplying Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db2b27",
   "metadata": {},
   "source": [
    "Below data augmentation is applied to increase the size of a training dataset. The implemented data augmentation methods include rotation and horizontal flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "671b90d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The samples in the dataset before augmentation were 80\n",
      "The samples in the dataset after augmentation are 560\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = augmentation(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5aeda4",
   "metadata": {},
   "source": [
    "# Splitting the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e67e18",
   "metadata": {},
   "source": [
    "Below, the training dataset is split into training and validation sets. A standard 80/20 split is employed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c92e63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "random_gen = torch.Generator().manual_seed(42) # find a random seed and fix it to always have the same split\n",
    "\n",
    "train_percent = 0.8\n",
    "train_size = int(train_percent * len(transformed_dataset))\n",
    "val_size = len(transformed_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(transformed_dataset, [train_size, val_size], random_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc5614",
   "metadata": {},
   "source": [
    "# Model summary and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91cc2a7",
   "metadata": {},
   "source": [
    "Now the CNN model is instantiated and a summary of the model is printed. Please refer to the '‘CNN_classes.py’' file within the CNN_model directory for further details on the defining of the CNN architecture. The model resembles a U-Net, consisting of an encoder and a corresponding decoder, connected by a bottleneck layer. In total, the model has 124,128,688 parameters, all of which are trainable.\n",
    "The key components of the model architecture are:\n",
    "\n",
    "\n",
    "<b>Encoder</b>\n",
    "\n",
    "The encoder consists of multiple layers of convolution and pooling operations, gradually reducing spatial dimensions while increasing feature channels:\n",
    "\n",
    "•\tThe model starts with a convolutional layer (Conv2d-1) followed by batch normalization (BatchNorm2d-2) and Rectified Linear Unit (ReLU-3).\n",
    "\n",
    "•\tThis is followed by another set of convolution, batch normalization, and ReLU (DoubleConv-7).\n",
    "\n",
    "•\tMax pooling (MaxPool2d-8) is applied to reduce spatial dimensions.\n",
    "\n",
    "•\tThe model then repeats the encoder structure, gradually increasing the number of channels in each block (Down-16, Down-25, Down-35).\n",
    "\n",
    "\n",
    "<b>Decoder</b>\n",
    "\n",
    "The decoder, comprised of up-sampling and convolutional layers, reconstructs the high-resolution segmentation map from the learned features:\n",
    "\n",
    "•\tThe decoder begins with upsampling using transposed convolution (ConvTranspose2d-44) followed by a series of convolution, batch normalization, and ReLU operations (DoubleConv-51).\n",
    "\n",
    "•\tThis process is repeated for each level of the decoder (Up-52, Up-61, Up-70, Up-79).\n",
    "\n",
    "\n",
    "<b>Skip connections</b>\n",
    "\n",
    "Skip connections are established between corresponding encoder and decoder layers, which enhance information flow between different resolutions; facilitating the preservation of fine details and improving the training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380a179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 64, 64]           4,608\n",
      "       BatchNorm2d-2          [-1, 128, 64, 64]             256\n",
      "              ReLU-3          [-1, 128, 64, 64]               0\n",
      "            Conv2d-4          [-1, 128, 64, 64]         147,456\n",
      "       BatchNorm2d-5          [-1, 128, 64, 64]             256\n",
      "              ReLU-6          [-1, 128, 64, 64]               0\n",
      "        DoubleConv-7          [-1, 128, 64, 64]               0\n",
      "         MaxPool2d-8          [-1, 128, 32, 32]               0\n",
      "            Conv2d-9          [-1, 256, 32, 32]         294,912\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "             ReLU-11          [-1, 256, 32, 32]               0\n",
      "           Conv2d-12          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-13          [-1, 256, 32, 32]             512\n",
      "             ReLU-14          [-1, 256, 32, 32]               0\n",
      "       DoubleConv-15          [-1, 256, 32, 32]               0\n",
      "             Down-16          [-1, 256, 32, 32]               0\n",
      "        MaxPool2d-17          [-1, 256, 16, 16]               0\n",
      "           Conv2d-18          [-1, 512, 16, 16]       1,179,648\n",
      "      BatchNorm2d-19          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-20          [-1, 512, 16, 16]               0\n",
      "           Conv2d-21          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-22          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-23          [-1, 512, 16, 16]               0\n",
      "       DoubleConv-24          [-1, 512, 16, 16]               0\n",
      "             Down-25          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-26            [-1, 512, 8, 8]               0\n",
      "           Conv2d-27           [-1, 1024, 8, 8]       4,718,592\n",
      "      BatchNorm2d-28           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-29           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-30           [-1, 1024, 8, 8]       9,437,184\n",
      "      BatchNorm2d-31           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-32           [-1, 1024, 8, 8]               0\n",
      "       DoubleConv-33           [-1, 1024, 8, 8]               0\n",
      "             Down-34           [-1, 1024, 8, 8]               0\n",
      "        MaxPool2d-35           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-36           [-1, 2048, 4, 4]      18,874,368\n",
      "      BatchNorm2d-37           [-1, 2048, 4, 4]           4,096\n",
      "             ReLU-38           [-1, 2048, 4, 4]               0\n",
      "           Conv2d-39           [-1, 2048, 4, 4]      37,748,736\n",
      "      BatchNorm2d-40           [-1, 2048, 4, 4]           4,096\n",
      "             ReLU-41           [-1, 2048, 4, 4]               0\n",
      "       DoubleConv-42           [-1, 2048, 4, 4]               0\n",
      "             Down-43           [-1, 2048, 4, 4]               0\n",
      "  ConvTranspose2d-44           [-1, 1024, 8, 8]       8,389,632\n",
      "           Conv2d-45           [-1, 1024, 8, 8]      18,874,368\n",
      "      BatchNorm2d-46           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-47           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-48           [-1, 1024, 8, 8]       9,437,184\n",
      "      BatchNorm2d-49           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-50           [-1, 1024, 8, 8]               0\n",
      "       DoubleConv-51           [-1, 1024, 8, 8]               0\n",
      "               Up-52           [-1, 1024, 8, 8]               0\n",
      "  ConvTranspose2d-53          [-1, 512, 16, 16]       2,097,664\n",
      "           Conv2d-54          [-1, 512, 16, 16]       4,718,592\n",
      "      BatchNorm2d-55          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-56          [-1, 512, 16, 16]               0\n",
      "           Conv2d-57          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-58          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-59          [-1, 512, 16, 16]               0\n",
      "       DoubleConv-60          [-1, 512, 16, 16]               0\n",
      "               Up-61          [-1, 512, 16, 16]               0\n",
      "  ConvTranspose2d-62          [-1, 256, 32, 32]         524,544\n",
      "           Conv2d-63          [-1, 256, 32, 32]       1,179,648\n",
      "      BatchNorm2d-64          [-1, 256, 32, 32]             512\n",
      "             ReLU-65          [-1, 256, 32, 32]               0\n",
      "           Conv2d-66          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-67          [-1, 256, 32, 32]             512\n",
      "             ReLU-68          [-1, 256, 32, 32]               0\n",
      "       DoubleConv-69          [-1, 256, 32, 32]               0\n",
      "               Up-70          [-1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-71          [-1, 128, 64, 64]         131,200\n",
      "           Conv2d-72          [-1, 128, 64, 64]         294,912\n",
      "      BatchNorm2d-73          [-1, 128, 64, 64]             256\n",
      "             ReLU-74          [-1, 128, 64, 64]               0\n",
      "           Conv2d-75          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-76          [-1, 128, 64, 64]             256\n",
      "             ReLU-77          [-1, 128, 64, 64]               0\n",
      "       DoubleConv-78          [-1, 128, 64, 64]               0\n",
      "               Up-79          [-1, 128, 64, 64]               0\n",
      "           Conv2d-80           [-1, 48, 64, 64]           6,192\n",
      "          OutConv-81           [-1, 48, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 124,128,688\n",
      "Trainable params: 124,128,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 130.38\n",
      "Params size (MB): 473.51\n",
      "Estimated Total Size (MB): 603.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CNN\n",
    "model = UNet().to(device)\n",
    "summary(model, input_size=(4, 64, 64)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38471c",
   "metadata": {},
   "source": [
    "Below, the model architecture is visualized using PyTorchViz. To turn on the visualization, make sure <code>visualize = True</code>\n",
    "\n",
    "PyTorchViz uses the following color-coding the model architecture graph:\n",
    "\n",
    "•\tBlue nodes represent tensors or variables in the computation graph. These are the data elements that flow through the operations.\n",
    "\n",
    "•\tGray nodes represent PyTorch functions or operations performed on tensors.\n",
    "\n",
    "•\tGreen nodes represent gradients or derivatives of tensors. They showcase the backpropagation flow of gradients through the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d77d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = False\n",
    "\n",
    "if visualize == True:\n",
    "    # Visualize the model architecture\n",
    "    dummy_input = torch.randn(1, 4, 64, 64).to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    out = model(dummy_input)\n",
    "    graph = make_dot(out, params=dict(model.named_parameters()))\n",
    "    output_path = os.path.join('images', 'CNN_model_graph')\n",
    "    graph.render(output_path, format=\"png\", cleanup=True)\n",
    "    img_path = f\"{output_path}.png\"\n",
    "    display(Image(filename=img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12dfb6d",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737eaea",
   "metadata": {},
   "source": [
    "<b>Training Parameters</b>\n",
    "\n",
    "First the learning rate, batch size, and the number of training epochs are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9527fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd8b72",
   "metadata": {},
   "source": [
    "<b>Optimizer and Dataloaders</b>\n",
    "\n",
    "An Adam optimizer is set up to train the neural network, and dataloaders are created to efficiently handle batches of data during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0cb13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False)\n",
    "test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8de020",
   "metadata": {},
   "source": [
    "<b>Training and Evaluation Loops</b>\n",
    "\n",
    "Here, we iterate over epochs, conducting both training and validation loops. Losses are computed and stored for each epoch, with periodic printing for monitoring. Early stopping is implemented to prevent overfitting and improve the generalization performance of a model.\n",
    "\n",
    "The code cell below uses two custom functions for training and evaluating:\n",
    "\n",
    "<b>train_epoch</b>\n",
    "\n",
    "The train_epoch function is used for training the model. It takes as input the model architecture (model), a data loader (loader) supplying training batches, an optimizer (optimizer) for weight updates, and the computation device (device). During each training epoch, the function iterates through the provided data loader, performs forward and backward passes, computes Mean Squared Error (MSE) loss, and updates the model parameters using backpropagation.\n",
    "\n",
    "<b>evaluation</b>\n",
    "\n",
    "The evaluation function is used for assessing the model's performance on a validation or test dataset. Similar to train_epoch, it takes the model (model), a data loader (loader), and the computation device (device) as inputs. However, it operates in evaluation mode, meaning it disables gradient computation and only performs forward passes to compute MSE loss on the validation or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cfe63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train() # specifies that the model is in training mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[0]\n",
    "        x = x[:, 0] # remove time step\n",
    "        y = batch[1]\n",
    "        y = y[:, :, 0] # remove part of the model that says singular feature\n",
    "        x, y = x.float().to(device), y.float().to(device)\n",
    "        # Model prediction\n",
    "        preds = model(x)\n",
    "\n",
    "        # MSE loss function\n",
    "        loss = nn.MSELoss()(preds, y)\n",
    "\n",
    "        losses.append(loss.cpu().detach())\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        loss.backward()   # compute the gradients using backpropagation\n",
    "        optimizer.step()  # update the weights with the optimizer\n",
    "        optimizer.zero_grad(set_to_none=True)   # reset the computed gradients\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluation(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval() # specifies that the model is in evaluation mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            x = x[:, 0] # remove time step\n",
    "            y = batch[1]\n",
    "            y = y[:, :, 0] # remove part of the model that says singular feature\n",
    "            x, y = x.float().to(device), y.float().to(device)\n",
    "\n",
    "            # Model prediction\n",
    "            preds = model(x)\n",
    "\n",
    "            # MSE loss function\n",
    "            loss = nn.MSELoss()(preds, y)\n",
    "            losses.append(loss.cpu().detach())\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e695b776-6b8f-46d2-92ec-925f93c90a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 112.90 seconds\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "best_validation_loss = float('inf')  # Initialize with a very large value\n",
    "patience = 25  # Number of epochs with no improvement after which training will be stopped\n",
    "\n",
    "start_time = time.time()  # Record the start time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device=device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    validation_loss = evaluation(model, val_loader, device=device)\n",
    "    validation_losses.append(validation_loss)\n",
    "\n",
    "    # Print the loss for every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Validation Loss: {validation_loss:.4f}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        patience_counter = 0  # Reset patience counter since there is an improvement\n",
    "    else:\n",
    "        patience_counter += 1  # No improvement, increase the patience counter\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1} | Best Validation Loss: {best_validation_loss:.4f}')\n",
    "        break  # Stop training\n",
    "\n",
    "end_time = time.time()  # Record the end time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f'Total time taken: {elapsed_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87291a97",
   "metadata": {},
   "source": [
    "<b> Test Set Evaluation and Loss Plotting</b>\n",
    "\n",
    "Below, the test loss is printed for the first and second test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "317f83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss using test dataset 1: 5295226400.0\n",
      "Test Loss using test dataset 2: 2601414000.0\n"
     ]
    }
   ],
   "source": [
    "test_loss_1 = evaluation(model, test_loader1, device=device)\n",
    "test_loss_2 = evaluation(model, test_loader2, device=device)\n",
    "print('Test Loss using test dataset 1:', test_loss_1)\n",
    "print('Test Loss using test dataset 2:', test_loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec03961-ead0-4853-95f0-c930436770e1",
   "metadata": {},
   "source": [
    "Below, the training and validation losses are plotted over epochs to visualize the learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03951c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training')\n",
    "plt.plot(validation_losses, label='Validation')\n",
    "plt.yscale('log')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf739f2",
   "metadata": {},
   "source": [
    "# Predicting Water Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f6ac6",
   "metadata": {},
   "source": [
    "Now that the model has been trained on the training dataset, it is used to make predictions of the water depth on unseen topographies using the test datasets. The predict_and_compare function is implemented to facilitate using different testing datasets. \n",
    "\n",
    "<b>predict_and_compare</b>\n",
    "\n",
    "This function takes a model, a dataset, a specific data ID, and a starting time step (t0) as input parameters. It then predicts water depth using the given model and compares the predictions with the ground truth.\n",
    "\n",
    "It returns the following NumPy arrays:\n",
    "\n",
    "•\tx_np represents the input data.\n",
    "\n",
    "•\tWD_np represents the ground truth water depth.\n",
    "\n",
    "•\tpred_WD_np represents the predicted water depth by the model.\n",
    "\n",
    "These variables are used to visualize the results in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db0926bf-aec1-4c2b-a02d-f341219a19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_compare(model, dataset, data_id, t0, device='cuda'):\n",
    "    # select one sample\n",
    "    x = dataset[data_id][0][0].float().unsqueeze(0).to(device)\n",
    "    WD = dataset[data_id][1].squeeze(1)\n",
    "\n",
    "    # Display the shapes of the input data and water depth tensor\n",
    "    print(f\"Shape of input data (x): {x.shape}\")\n",
    "    print(f\"Shape of water depth (WD): {WD.shape}\")\n",
    "\n",
    "    # Define the input tensor (x)\n",
    "    x_np = x.squeeze(0).cpu().numpy()  # Convert tensor to NumPy array and remove the batch dimension\n",
    "\n",
    "    # Define the WD tensor\n",
    "    WD_np = WD.cpu().numpy()  # Convert tensor to NumPy array\n",
    "\n",
    "    # Assuming WD is a sequence of images (96 time steps)\n",
    "    num_time_steps = WD_np.shape[0]\n",
    "\n",
    "    # Plotting specific time steps (e.g., every 10th time step)\n",
    "    time_steps_to_plot = list(range(0, num_time_steps, 10))\n",
    "\n",
    "    # predict the WD\n",
    "    pred_WD = model(x).detach()\n",
    "\n",
    "    # Convert the predicted tensor to numpy array\n",
    "    pred_WD_np = pred_WD.squeeze(0).cpu().numpy()  # Assuming batch dimension needs to be squeezed\n",
    "\n",
    "    return x_np, WD_np, pred_WD_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37dd216-c404-49f0-985d-c77c952e228d",
   "metadata": {},
   "source": [
    "Below, the predict_and_compare function is called using test_dataset1 and test_dataset2, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0134daa-e13e-45ba-af52-70159afa6b75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_np1, WD_np1, pred_WD_np1 \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_and_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m x_np2, WD_np2, pred_WD_np2 \u001b[38;5;241m=\u001b[39m predict_and_compare(model, test_dataset2, data_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, t0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m, in \u001b[0;36mpredict_and_compare\u001b[1;34m(model, dataset, data_id, t0, device)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_and_compare\u001b[39m(model, dataset, data_id, t0, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# select one sample\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     WD \u001b[38;5;241m=\u001b[39m dataset[data_id][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Display the shapes of the input data and water depth tensor\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\dsaie\\lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "x_np1, WD_np1, pred_WD_np1 = predict_and_compare(model, test_dataset1, data_id=7, t0=0, device='cuda')\n",
    "x_np2, WD_np2, pred_WD_np2 = predict_and_compare(model, test_dataset2, data_id=7, t0=0, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff7c19-9762-429b-9766-3b0451b7fed5",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdb4ef-5314-47df-89b7-f32a12c79d3f",
   "metadata": {},
   "source": [
    "Now the results are visualized. The plots below are made using the create_combined_gif function, which creates a combined GIF with multiple subplots for the actual, predicted, and difference arrays, along with a subplot for the DEM map. It visualizes the evolution of these arrays over time.\n",
    "\n",
    "The subplots include:\n",
    "\n",
    "•\tSubplot 1: DEM Map\n",
    "\n",
    "•\tSubplot 2: Actual data at time t\n",
    "\n",
    "•\tSubplot 3: Predicted data at time t\n",
    "\n",
    "•\tSubplot 4: Difference between actual and predicted data at time t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfe9c6-0dc4-4050-adc6-627201fc67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_combined_gif(actual_array, predicted_array, difference_array, dem_map, title, dataset_name, cmap='Blues', loop=0, figsize=(4, 4)):\n",
    "#     \"\"\"\n",
    "#     Creates a combined GIF with DEM map and multiple subplots for actual, predicted, and difference arrays.\n",
    "\n",
    "#     Parameters:\n",
    "#     - actual_array (numpy.ndarray): The actual data array.\n",
    "#     - predicted_array (numpy.ndarray): The predicted data array.\n",
    "#     - difference_array (numpy.ndarray): The difference data array.\n",
    "#     - dem_map (numpy.ndarray): The DEM (Digital Elevation Map) array.\n",
    "#     - title (str): Title for the GIF and subplots.\n",
    "#     - dataset_name (str): Name of the dataset for saving the GIF file.\n",
    "#     - cmap (str): Colormap for actual and predicted arrays. Default is 'Blues'.\n",
    "#     - loop (int): Number of times to loop the GIF. Default is 0 (no loop).\n",
    "#     - figsize (tuple): Figure size for subplots. Default is (4, 4).\n",
    "\n",
    "#     Returns:\n",
    "#     - str: Filename of the saved GIF.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Calculate the constant min and max values for the color scale\n",
    "#     min_value = min(difference_array.min(), 0)  # Minimum value set to 0\n",
    "#     max_value = max(actual_array.max(), predicted_array.max(), difference_array.max())\n",
    "\n",
    "#     images = []\n",
    "#     for t in range(actual_array.shape[0]):\n",
    "#         fig, axs = plt.subplots(1, 4, figsize=(4 * figsize[0], figsize[1]))  # Create four subplots side by side\n",
    "#         im_dem = axs[0].imshow(dem_map, cmap=colormaps[0], origin='lower', vmin=min_value, vmax=max_value)  # Display DEM map\n",
    "#         im_actual = axs[1].imshow(actual_array[t], cmap=cmap, origin='lower', vmin=0, vmax=max_value)  # Start from 0 as white\n",
    "#         im_predicted = axs[2].imshow(predicted_array[t], cmap=cmap, origin='lower', vmin=0, vmax=max_value)  # Start from 0 as white  \n",
    "\n",
    "#         # changed im_difference to make sure the scale is symmetric around zero\n",
    "#         im_difference = axs[3].imshow(difference_array[t], cmap='seismic', origin='lower', vmin=-max_value, vmax=max_value)  # Using a specific colormap for difference\n",
    "\n",
    "#         fig.colorbar(im_dem, ax=axs[0], fraction=0.046, pad=0.04)\n",
    "#         fig.colorbar(im_actual, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "#         fig.colorbar(im_predicted, ax=axs[2], fraction=0.046, pad=0.04)\n",
    "#         fig.colorbar(im_difference, ax=axs[3], fraction=0.046, pad=0.04)\n",
    "\n",
    "#         axs[0].set_title('DEM Map')  # Title for DEM map\n",
    "#         axs[1].set_title(f\"Actual {title} {t+1}\")\n",
    "#         axs[2].set_title(f\"Predicted {title} {t+1}\")\n",
    "#         axs[3].set_title(f\"Difference {title} {t+1}\")\n",
    "\n",
    "#         for ax in axs:\n",
    "#             ax.axis('off')\n",
    "\n",
    "#         plt.tight_layout()\n",
    "\n",
    "#         # Convert the figure to an image in memory\n",
    "#         buf = io.BytesIO()\n",
    "#         plt.savefig(buf, format='png')\n",
    "#         buf.seek(0)\n",
    "#         images.append(imageio.imread(buf))\n",
    "#         plt.close()\n",
    "\n",
    "#     # Specify the folder for saving the GIF\n",
    "#     save_folder = 'post_processing'\n",
    "#     os.makedirs(save_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "#     gif_filename = os.path.join(save_folder, f'combined_{title.replace(\" \", \"_\").lower()}_{dataset_name}.gif')\n",
    "#     with imageio.get_writer(gif_filename, mode='I', loop=loop) as writer:\n",
    "#         for image in images:\n",
    "#             writer.append_data(image)\n",
    "\n",
    "#     return gif_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5091c76-9227-4d96-a2e8-39d419d7a2c7",
   "metadata": {},
   "source": [
    "for test dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffe96c-a68a-4aae-9b0d-39ec7f598f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name1 = 'test_dataset1'\n",
    "combined_gif_with_dem1 = create_combined_gif(WD_np1, pred_WD_np1, WD_np1 - pred_WD_np1, x_np1[0], 'Water Depth', dataset_name1, figsize=(3, 3))\n",
    "display(Image(filename=combined_gif_with_dem1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072e60f-0b2d-4673-b2df-29898c16ebc8",
   "metadata": {},
   "source": [
    "for test dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e4c06-52eb-47f7-8030-236af76f9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name2 = 'test_dataset2'\n",
    "combined_gif_with_dem2 = create_combined_gif(WD_np2, pred_WD_np2, WD_np2 - pred_WD_np2, x_np2[0], 'Water Depth', dataset_name2, figsize=(3, 3))\n",
    "display(Image(filename=combined_gif_with_dem2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2c167-6cb1-442d-bfcd-f734984b7407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
