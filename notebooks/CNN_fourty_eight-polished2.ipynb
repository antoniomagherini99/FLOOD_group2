{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66242d2f",
   "metadata": {},
   "source": [
    "# CNN model notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db14a1",
   "metadata": {},
   "source": [
    "Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5de71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\Documents\\GitHub\\FLOOD_group2\n"
     ]
    }
   ],
   "source": [
    "# move to the root directory of the git\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df834647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import imageio\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import random\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from pre_processing.encode_decode_csv import decode_from_csv\n",
    "\n",
    "from pre_processing.process_data import *\n",
    "from models.CNN_model.CNN_functions import *\n",
    "from models.CNN_model.CNN_classes import *\n",
    "from pre_processing.augmentation import *\n",
    "from pre_processing.normalization import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ccc13",
   "metadata": {},
   "source": [
    "# Creating Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc77c2",
   "metadata": {},
   "source": [
    "First the training dataset is created based upon a research paper by Bentivoglio et al (2023). For training and validation purposes a dataset of 80 different generated digital elevation models are used to reflect different plausible topographies. The velocity in both x and y direction (VX and VY) as well as the water depth (WD) are known, based on numerical computations utilizing Delft3D.\n",
    "\n",
    "The code cell below uses two custom functions for the elevation data processing and water depth processing. \n",
    "\n",
    "<b>Elevation Data Processing</b>\n",
    "\n",
    "The process_elevation_data function takes a DEM (Digital Elevation Model) file identifier (file_id) and a dataset identifier (dataset_id) as inputs. It reads the corresponding DEM file, extracts elevation data, and calculates the slope in the x and y directions. The result is a torch tensor combining the original elevation data with its slope information.\n",
    "\n",
    "<b>Water Depth Processing</b>\n",
    "\n",
    "The process_water_depth function processes water depth data from a specific time step in a file. It requires a water depth file identifier (file_id), a dataset identifier (dataset_id), and an optional time step parameter (time_step, default is 0). The function reads the specified file, extracts the data for the given time step, and returns a 64x64 torch tensor representing water depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8168f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the time t0 and t\n",
    "# t0 = 0\n",
    "# t = 1\n",
    "\n",
    "# train_dataset = []\n",
    "\n",
    "# for i in range(1, 81):  # Loop through file IDs from DEM_1 to DEM_80\n",
    "#     file_id = i\n",
    "    \n",
    "#     # Input Tensor (input_tensor.shape will be [4, 64, 64])\n",
    "#     elevation_slope_tensor = process_elevation_data(file_id, 'train/val')\n",
    "#     water_depth_input_tensor = process_water_depth(file_id, 'train/val', time_step=t0)  # Time Step is t0\n",
    "#     water_depth_input_tensor = torch.unsqueeze(water_depth_input_tensor, 0)\n",
    "    \n",
    "#     input_tensor = torch.cat((elevation_slope_tensor, water_depth_input_tensor), dim=0)\n",
    "#     input_tensor = input_tensor.double()\n",
    "    \n",
    "#     # Output Sequence Tensor (output_sequence_tensor.shape will be [48, 64, 64])\n",
    "#     output_tensors = []\n",
    "#     for time_step_index in range(1, 96, 2):  # Loop through every even time step out of the 96\n",
    "#         water_depth_output_tensor = process_water_depth(file_id, 'train/val', time_step=t0 + time_step_index)\n",
    "#         output_tensors.append(water_depth_output_tensor)\n",
    "    \n",
    "#     output_sequence_tensor = torch.stack(output_tensors, dim=0)\n",
    "#     output_sequence_tensor = output_sequence_tensor.double()\n",
    "    \n",
    "#     # Create a tuple\n",
    "#     train_dataset_sample = (input_tensor, output_sequence_tensor)\n",
    "    \n",
    "#     # Append the sample to the train_dataset list\n",
    "#     train_dataset.append(train_dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d940397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing a few samples to verify the shape of train input and output tensors\n",
    "\n",
    "# for i, sample in enumerate(train_dataset[:5]):  # Print shapes for the first 5 samples\n",
    "#     input_train_tensor, output_train_sequence_tensor = sample\n",
    "    \n",
    "#     print(f\"Sample {i+1}:\")\n",
    "#     print(\"Input Train Tensor Shape:\", input_train_tensor.shape)\n",
    "#     print(\"Output Train Sequence Tensor Shape:\", output_train_sequence_tensor.shape)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa540ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored inputs Shape: torch.Size([80, 1, 4, 64, 64])\n",
      "Restored targets Shape: torch.Size([80, 48, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# David test\n",
    "# Trying out Lucas's decoder function\n",
    "def remove_discharge(train_val_test):\n",
    "    '''\n",
    "    Print is located within decode_csv\n",
    "    '''\n",
    "    dataset = decode_from_csv(train_val_test)\n",
    "    inputs = dataset[:][0]\n",
    "    targets = dataset[:][1]\n",
    "    targets = targets[:,:, 0].unsqueeze (2) # remove discharge\n",
    "    new_dataset = TensorDataset(inputs.float(), targets.float())\n",
    "    return new_dataset\n",
    "train_dataset = remove_discharge('train_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7bb7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select a random sample from the dataset\n",
    "# sample_index = random.randint(0, len(train_dataset) - 1)\n",
    "# print(sample_index)\n",
    "# input_tensor, output_sequence_tensor = train_dataset[sample_index]\n",
    "\n",
    "# # Plot input tensors (DEM, slope x, slope y, water depth at initial time step)\n",
    "# fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "# # Plot DEM (Terrain) with appropriate colormap\n",
    "# axs[0].imshow(input_tensor[0], cmap='terrain')\n",
    "# axs[0].set_title('DEM (Terrain)')\n",
    "\n",
    "# # Plot slope x and slope y with 'coolwarm' colormap\n",
    "# for i in range(1, 3):\n",
    "#     axs[i].imshow(input_tensor[i], cmap='coolwarm')\n",
    "#     axs[i].set_title(f'Slope {\"X\" if i == 1 else \"Y\"}')\n",
    "\n",
    "# # Plot water depth at initial time step and output at a random time step with 'Blues' colormap\n",
    "# axs[3].imshow(input_tensor[3], cmap='Blues')\n",
    "# axs[3].set_title('Water Depth (Initial Time Step)')\n",
    "\n",
    "# # Select a random time step from the output sequence tensor\n",
    "# random_time_step = random.randint(0, output_sequence_tensor.shape[0] - 1)\n",
    "# output_at_time_step = output_sequence_tensor[random_time_step]\n",
    "\n",
    "# # Plot the output at the random time step\n",
    "# axs[4].imshow(output_at_time_step, cmap='Blues')\n",
    "# axs[4].set_title(f'Output at Time Step {random_time_step}')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b9533",
   "metadata": {},
   "source": [
    "# Creating the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ef268",
   "metadata": {},
   "source": [
    "Now the test dataset is created based on dataset1 from Bentivoglio et al (2023). This dataset features 20 DEMs over a squared domain of 64x64 grids of length 100 m and a simulation time of 48 h. A fixed breach location is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ddedcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the time t0 and t\n",
    "# t0 = 0\n",
    "# t = 1\n",
    "\n",
    "# # Loop through file IDs from DEM_1 to DEM_80\n",
    "# test_dataset = []\n",
    "\n",
    "# for i in range(500, 520):  # Assuming file IDs are numbered from 500 to 520\n",
    "#     file_id = i\n",
    "    \n",
    "#     # Input Tensor (input_tensor.shape will be [4, 64, 64])\n",
    "#     elevation_slope_tensor = process_elevation_data(file_id, 'dataset1')\n",
    "#     water_depth_input_tensor = process_water_depth(file_id, 'dataset1', time_step=t0)  # Time Step is t0\n",
    "#     # Add an extra dimension to make water_depth_tensor [1, 64, 64]\n",
    "#     water_depth_input_tensor = torch.unsqueeze(water_depth_input_tensor, 0)\n",
    "    \n",
    "#     # Concatenate to create the input tensor\n",
    "#     input_test_tensor = torch.cat((elevation_slope_tensor, water_depth_input_tensor), dim=0)\n",
    "#     input_test_tensor = input_test_tensor.double()\n",
    "    \n",
    "#     # Output Sequence Tensor (output_sequence_tensor.shape will be [48, 64, 64])\n",
    "#     output_test_tensors = []\n",
    "#     for time_step_index in range(1,96,2):  # Loop through every even time step out of the 96\n",
    "#         water_depth_output_tensor = process_water_depth(file_id, 'dataset1', time_step=t0 + time_step_index)\n",
    "#         output_test_tensors.append(water_depth_output_tensor)\n",
    "    \n",
    "#     # Stack the list of output tensors along the new time dimension to create a sequence\n",
    "#     output_test_sequence_tensor = torch.stack(output_test_tensors, dim=0)\n",
    "#     output_test_sequence_tensor = output_test_sequence_tensor.double()\n",
    "    \n",
    "#     # Create a tuple\n",
    "#     test_dataset_sample = (input_test_tensor, output_test_sequence_tensor)\n",
    "    \n",
    "#     # Append the sample to the test_dataset list\n",
    "#     test_dataset.append(test_dataset_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a7ec89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing a few samples to verify the shape of test input and output tensors\n",
    "\n",
    "# for i, sample in enumerate(test_dataset[:5]):  # Print shapes for the first 5 samples\n",
    "#     input_test_tensor, output_test_sequence_tensor = sample\n",
    "    \n",
    "#     print(f\"Sample {i+1}:\")\n",
    "#     print(\"Input Test Tensor Shape:\", input_test_tensor.shape)\n",
    "#     print(\"Output Test Sequence Tensor Shape:\", output_test_sequence_tensor.shape)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8188290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select a random sample from the dataset\n",
    "# sample_index = random.randint(0, len(test_dataset) - 1)\n",
    "# input_test_tensor, output_test_sequence_tensor = test_dataset[sample_index]\n",
    "\n",
    "# # Plot input tensors (DEM, slope x, slope y, water depth at initial time step)\n",
    "# fig, axs = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "# # Plot DEM (Terrain) with appropriate colormap\n",
    "# axs[0].imshow(input_tensor[0], cmap='terrain')\n",
    "# axs[0].set_title('DEM (Terrain)')\n",
    "\n",
    "# # Plot slope x and slope y with 'coolwarm' colormap\n",
    "# for i in range(1, 3):\n",
    "#     axs[i].imshow(input_test_tensor[i], cmap='coolwarm')\n",
    "#     axs[i].set_title(f'Slope {\"X\" if i == 1 else \"Y\"}')\n",
    "\n",
    "# # Plot water depth at initial time step and output at a random time step with 'Blues' colormap\n",
    "# axs[3].imshow(input_test_tensor[3], cmap='Blues')\n",
    "# axs[3].set_title('Water Depth (Initial Time Step)')\n",
    "\n",
    "# # Select a random time step from the output sequence tensor\n",
    "# random_time_step = random.randint(0, output_sequence_tensor.shape[0] - 1)\n",
    "# output_test_at_time_step = output_test_sequence_tensor[random_time_step]\n",
    "\n",
    "# # Plot the output at the random time step\n",
    "# axs[4].imshow(output_test_at_time_step, cmap='Blues')\n",
    "# axs[4].set_title(f'Output at Time Step {random_time_step}')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc4f40b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored inputs Shape: torch.Size([20, 1, 4, 64, 64])\n",
      "Restored targets Shape: torch.Size([20, 48, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test_dataset = remove_discharge('test1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e01e5",
   "metadata": {},
   "source": [
    "# Aplying Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "671b90d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs sizes: torch.Size([1, 4, 64, 64]),\n",
      "Outputs sizes: torch.Size([48, 1, 64, 64])\n",
      "\n",
      "rotate concat: <class 'torch.Tensor'>\n",
      "The samples in the dataset before augmentation were 80\n",
      "The samples in the dataset after augmentation are 160\n"
     ]
    }
   ],
   "source": [
    "# apply augmentation\n",
    "transformed_dataset = augmentation(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5aeda4",
   "metadata": {},
   "source": [
    "# Splitting the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92e63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "random_gen = torch.Generator().manual_seed(42) # find a random seed and fix it to always have the same split\n",
    "\n",
    "train_percent = 0.8\n",
    "train_size = int(train_percent * len(transformed_dataset))\n",
    "val_size = len(transformed_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(transformed_dataset, [train_size, val_size], random_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32388766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the inputs and outputs using training dataset\n",
    "scaler_x, scaler_y = scaler(train_dataset)\n",
    "\n",
    "normalized_train_dataset = normalize_dataset(train_dataset, scaler_x, scaler_y, 'train_val')\n",
    "normalized_val_dataset = normalize_dataset(val_dataset, scaler_x, scaler_y, 'train_val')\n",
    "norm_test_dataset = normalize_dataset(test_dataset, scaler_x, scaler_y, 'test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a2ceb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_dataset(dataset, scaler_x, scaler_y):\n",
    "#     # Extract min and max values from the scalers for inputs and outputs\n",
    "#     min_x, max_x = scaler_x.data_min_[0], scaler_x.data_max_[0]\n",
    "#     min_y, max_y = scaler_y.data_min_[0], scaler_y.data_max_[0]\n",
    "\n",
    "#     normalized_dataset = []\n",
    "#     for idx in range(len(dataset)):\n",
    "#         x = dataset[idx][0]\n",
    "#         y_sequence = dataset[idx][1]\n",
    "\n",
    "#         # Normalize input (assuming it's a single tensor)\n",
    "#         norm_x = (x - min_x) / (max_x - min_x)\n",
    "\n",
    "#         # Normalize each output sequence tensor individually\n",
    "#         normalized_sequence = []\n",
    "#         for t in range(y_sequence.shape[0]):\n",
    "#             norm_y = (y_sequence[t] - min_y) / (max_y - min_y)\n",
    "#             normalized_sequence.append(norm_y)\n",
    "\n",
    "#         normalized_dataset.append((norm_x, torch.stack(normalized_sequence)))\n",
    "\n",
    "#     return normalized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e761bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_x = MinMaxScaler()\n",
    "# scaler_y = MinMaxScaler()\n",
    "\n",
    "# # Fit the scalers using the training dataset\n",
    "# for idx in range(len(train_dataset)):\n",
    "#     scaler_x.partial_fit(train_dataset[idx][0].flatten().unsqueeze(0).T.cpu())\n",
    "#     scaler_y.partial_fit(train_dataset[idx][1].flatten().unsqueeze(0).T.cpu())\n",
    "\n",
    "# # Normalize the datasets\n",
    "# normalized_train_dataset = normalize_dataset(train_dataset, scaler_x, scaler_y)\n",
    "# normalized_test_dataset = normalize_dataset(test_dataset, scaler_x, scaler_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59f06cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming normalized_train_dataset and normalized_test_dataset are your normalized datasets\n",
    "\n",
    "# # Print the minimum and maximum values for input and output sequences in the first sample of the normalized training dataset\n",
    "# input_sample, output_sample = normalized_train_dataset[0]\n",
    "# print(\"Input Min-Max:\", input_sample[0].min().item(), \"-\", input_sample[0].max().item())\n",
    "# print(\"Output Min-Max:\", output_sample.min().item(), \"-\", output_sample.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab46603",
   "metadata": {},
   "source": [
    "Below, the training dataset is split into training and validation sets. A standard 80/20 split is employed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b95a6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_percent = 0.8\n",
    "# train_size = int(train_percent * len(train_dataset))\n",
    "# val_size = len(train_dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc5614",
   "metadata": {},
   "source": [
    "# Model summary and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91cc2a7",
   "metadata": {},
   "source": [
    "Now the CNN model is instantiated and a summary of the model is printed. Please refer to the '‘CNN_classes.py’' file within the CNN_model directory for further details on the defining of the CNN architecture. The model resembles a U-Net, consisting of an encoder and a corresponding decoder, connected by a bottleneck layer. In total, the model has 124,128,688 parameters, all of which are trainable.\n",
    "The key components of the model architecture are:\n",
    "\n",
    "\n",
    "<b>Encoder</b>\n",
    "\n",
    "The encoder consists of multiple layers of convolution and pooling operations, gradually reducing spatial dimensions while increasing feature channels:\n",
    "\n",
    "•\tThe model starts with a convolutional layer (Conv2d-1) followed by batch normalization (BatchNorm2d-2) and Rectified Linear Unit (ReLU-3).\n",
    "\n",
    "•\tThis is followed by another set of convolution, batch normalization, and ReLU (DoubleConv-7).\n",
    "\n",
    "•\tMax pooling (MaxPool2d-8) is applied to reduce spatial dimensions.\n",
    "\n",
    "•\tThe model then repeats the encoder structure, gradually increasing the number of channels in each block (Down-16, Down-25, Down-35).\n",
    "\n",
    "\n",
    "<b>Decoder</b>\n",
    "\n",
    "The decoder, comprised of up-sampling and convolutional layers, reconstructs the high-resolution segmentation map from the learned features:\n",
    "\n",
    "•\tThe decoder begins with upsampling using transposed convolution (ConvTranspose2d-44) followed by a series of convolution, batch normalization, and ReLU operations (DoubleConv-51).\n",
    "\n",
    "•\tThis process is repeated for each level of the decoder (Up-52, Up-61, Up-70, Up-79).\n",
    "\n",
    "\n",
    "<b>Skip connections</b>\n",
    "\n",
    "Skip connections are established between corresponding encoder and decoder layers, which enhance information flow between different resolutions; facilitating the preservation of fine details and improving the training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "380a179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 64, 64]           4,608\n",
      "       BatchNorm2d-2          [-1, 128, 64, 64]             256\n",
      "              ReLU-3          [-1, 128, 64, 64]               0\n",
      "            Conv2d-4          [-1, 128, 64, 64]         147,456\n",
      "       BatchNorm2d-5          [-1, 128, 64, 64]             256\n",
      "              ReLU-6          [-1, 128, 64, 64]               0\n",
      "        DoubleConv-7          [-1, 128, 64, 64]               0\n",
      "         MaxPool2d-8          [-1, 128, 32, 32]               0\n",
      "            Conv2d-9          [-1, 256, 32, 32]         294,912\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "             ReLU-11          [-1, 256, 32, 32]               0\n",
      "           Conv2d-12          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-13          [-1, 256, 32, 32]             512\n",
      "             ReLU-14          [-1, 256, 32, 32]               0\n",
      "       DoubleConv-15          [-1, 256, 32, 32]               0\n",
      "             Down-16          [-1, 256, 32, 32]               0\n",
      "        MaxPool2d-17          [-1, 256, 16, 16]               0\n",
      "           Conv2d-18          [-1, 512, 16, 16]       1,179,648\n",
      "      BatchNorm2d-19          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-20          [-1, 512, 16, 16]               0\n",
      "           Conv2d-21          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-22          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-23          [-1, 512, 16, 16]               0\n",
      "       DoubleConv-24          [-1, 512, 16, 16]               0\n",
      "             Down-25          [-1, 512, 16, 16]               0\n",
      "        MaxPool2d-26            [-1, 512, 8, 8]               0\n",
      "           Conv2d-27           [-1, 1024, 8, 8]       4,718,592\n",
      "      BatchNorm2d-28           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-29           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-30           [-1, 1024, 8, 8]       9,437,184\n",
      "      BatchNorm2d-31           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-32           [-1, 1024, 8, 8]               0\n",
      "       DoubleConv-33           [-1, 1024, 8, 8]               0\n",
      "             Down-34           [-1, 1024, 8, 8]               0\n",
      "        MaxPool2d-35           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-36           [-1, 2048, 4, 4]      18,874,368\n",
      "      BatchNorm2d-37           [-1, 2048, 4, 4]           4,096\n",
      "             ReLU-38           [-1, 2048, 4, 4]               0\n",
      "           Conv2d-39           [-1, 2048, 4, 4]      37,748,736\n",
      "      BatchNorm2d-40           [-1, 2048, 4, 4]           4,096\n",
      "             ReLU-41           [-1, 2048, 4, 4]               0\n",
      "       DoubleConv-42           [-1, 2048, 4, 4]               0\n",
      "             Down-43           [-1, 2048, 4, 4]               0\n",
      "  ConvTranspose2d-44           [-1, 1024, 8, 8]       8,389,632\n",
      "           Conv2d-45           [-1, 1024, 8, 8]      18,874,368\n",
      "      BatchNorm2d-46           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-47           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-48           [-1, 1024, 8, 8]       9,437,184\n",
      "      BatchNorm2d-49           [-1, 1024, 8, 8]           2,048\n",
      "             ReLU-50           [-1, 1024, 8, 8]               0\n",
      "       DoubleConv-51           [-1, 1024, 8, 8]               0\n",
      "               Up-52           [-1, 1024, 8, 8]               0\n",
      "  ConvTranspose2d-53          [-1, 512, 16, 16]       2,097,664\n",
      "           Conv2d-54          [-1, 512, 16, 16]       4,718,592\n",
      "      BatchNorm2d-55          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-56          [-1, 512, 16, 16]               0\n",
      "           Conv2d-57          [-1, 512, 16, 16]       2,359,296\n",
      "      BatchNorm2d-58          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-59          [-1, 512, 16, 16]               0\n",
      "       DoubleConv-60          [-1, 512, 16, 16]               0\n",
      "               Up-61          [-1, 512, 16, 16]               0\n",
      "  ConvTranspose2d-62          [-1, 256, 32, 32]         524,544\n",
      "           Conv2d-63          [-1, 256, 32, 32]       1,179,648\n",
      "      BatchNorm2d-64          [-1, 256, 32, 32]             512\n",
      "             ReLU-65          [-1, 256, 32, 32]               0\n",
      "           Conv2d-66          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-67          [-1, 256, 32, 32]             512\n",
      "             ReLU-68          [-1, 256, 32, 32]               0\n",
      "       DoubleConv-69          [-1, 256, 32, 32]               0\n",
      "               Up-70          [-1, 256, 32, 32]               0\n",
      "  ConvTranspose2d-71          [-1, 128, 64, 64]         131,200\n",
      "           Conv2d-72          [-1, 128, 64, 64]         294,912\n",
      "      BatchNorm2d-73          [-1, 128, 64, 64]             256\n",
      "             ReLU-74          [-1, 128, 64, 64]               0\n",
      "           Conv2d-75          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-76          [-1, 128, 64, 64]             256\n",
      "             ReLU-77          [-1, 128, 64, 64]               0\n",
      "       DoubleConv-78          [-1, 128, 64, 64]               0\n",
      "               Up-79          [-1, 128, 64, 64]               0\n",
      "           Conv2d-80           [-1, 48, 64, 64]           6,192\n",
      "          OutConv-81           [-1, 48, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 124,128,688\n",
      "Trainable params: 124,128,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 130.38\n",
      "Params size (MB): 473.51\n",
      "Estimated Total Size (MB): 603.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CNN\n",
    "model = UNet().to(device)\n",
    "summary(model, input_size=(4, 64, 64)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38471c",
   "metadata": {},
   "source": [
    "Below, the model architecture is visualized using PyTorchViz. PyTorchViz uses the following color-coding the model architecture graph:\n",
    "\n",
    "•\tBlue nodes represent tensors or variables in the computation graph. These are the data elements that flow through the operations.\n",
    "\n",
    "•\tGray nodes represent PyTorch functions or operations performed on tensors.\n",
    "\n",
    "•\tGreen nodes represent gradients or derivatives of tensors. They showcase the backpropagation flow of gradients through the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d77d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model architecture\n",
    "dummy_input = torch.randn(1, 4, 64, 64).to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "out = model(dummy_input)\n",
    "graph = make_dot(out, params=dict(model.named_parameters()))\n",
    "output_path = os.path.join('images', 'CNN_model_graph')\n",
    "graph.render(output_path, format=\"png\", cleanup=True)\n",
    "img_path = f\"{output_path}.png\"\n",
    "display(Image(filename=img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12dfb6d",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737eaea",
   "metadata": {},
   "source": [
    "<b>Training Parameters</b>\n",
    "\n",
    "First the learning rate, batch size, and the number of training epochs are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9527fb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd8b72",
   "metadata": {},
   "source": [
    "<b>Optimizer and Dataloaders</b>\n",
    "\n",
    "An Adam optimizer is set up to train the neural network, and dataloaders are created to efficiently handle batches of data during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0cb13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(normalized_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(normalized_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(norm_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa083c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 1, 64, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8de020",
   "metadata": {},
   "source": [
    "<b>Training and Evaluation Loops</b>\n",
    "\n",
    "Here, we iterate over epochs, conducting both training and validation loops. Losses are computed and stored for each epoch, with periodic printing for monitoring.\n",
    "\n",
    "The code cell below uses two custom functions for training and evaluating:\n",
    "\n",
    "<b>train_epoch</b>\n",
    "\n",
    "The train_epoch function is used for training the model. It takes as input the model architecture (model), a data loader (loader) supplying training batches, an optimizer (optimizer) for weight updates, and the computation device (device). During each training epoch, the function iterates through the provided data loader, performs forward and backward passes, computes Mean Squared Error (MSE) loss, and updates the model parameters using backpropagation.\n",
    "\n",
    "<b>evaluation</b>\n",
    "\n",
    "The evaluation function is used for assessing the model's performance on a validation or test dataset. Similar to train_epoch, it takes the model (model), a data loader (loader), and the computation device (device) as inputs. However, it operates in evaluation mode, meaning it disables gradient computation and only performs forward passes to compute MSE loss on the validation or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cfe63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train() # specifies that the model is in training mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[0]\n",
    "        x = x[:, 0] # remove time step\n",
    "        y = batch[1]\n",
    "        y = y[:, :, 0] # remove part of the model that says singular feature\n",
    "        x, y = x.float().to(device), y.float().to(device)\n",
    "        # Model prediction\n",
    "        preds = model(x)\n",
    "\n",
    "        # MSE loss function\n",
    "        loss = nn.MSELoss()(preds, y)\n",
    "\n",
    "        losses.append(loss.cpu().detach())\n",
    "\n",
    "        # Backpropagate and update weights\n",
    "        loss.backward()   # compute the gradients using backpropagation\n",
    "        optimizer.step()  # update the weights with the optimizer\n",
    "        optimizer.zero_grad(set_to_none=True)   # reset the computed gradients\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluation(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval() # specifies that the model is in evaluation mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            x = x[:, 0] # remove time step\n",
    "            y = batch[1]\n",
    "            y = y[:, :, 0] # remove part of the model that says singular feature\n",
    "            x, y = x.float().to(device), y.float().to(device)\n",
    "\n",
    "            # Model prediction\n",
    "            preds = model(x)\n",
    "\n",
    "            # MSE loss function\n",
    "            loss = nn.MSELoss()(preds, y)\n",
    "            losses.append(loss.cpu().detach())\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0421294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device=device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    validation_loss = evaluation(model, val_loader, device=device)\n",
    "    validation_losses.append(validation_loss)\n",
    "\n",
    "    # Print the loss for every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Validation Loss: {validation_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87291a97",
   "metadata": {},
   "source": [
    "<b> Test Set Evaluation and Loss Plotting</b>\n",
    "\n",
    "Below, the model is evaluated on the test set and the test loss is printed. Additionally, the training and validation losses are plotted over epochs to visualize the learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluation(model, test_loader, device=device)\n",
    "print('Test Loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03951c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training')\n",
    "plt.plot(validation_losses, label='Validation')\n",
    "plt.yscale('log')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf739f2",
   "metadata": {},
   "source": [
    "# Visualizing Test Sample and Predicted Water Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeda9e5",
   "metadata": {},
   "source": [
    "<b>Selecting a Test Sample</b>\n",
    "\n",
    "Here, a specific test sample is selected (data_id = 2) from the test dataset. The input data (x) and the ground truth water depth (WD) are prepared for visualization. Also, the shapes of the input data tensor (x) and the water depth tensor (WD) is printed for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22348abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one sample\n",
    "data_id = 2\n",
    "\n",
    "x = test_dataset[data_id][0].unsqueeze(0)\n",
    "x = x.float().to(device)\n",
    "WD = test_dataset[data_id][1]\n",
    "\n",
    "# Display the shapes of the input data and water depth tensor\n",
    "print(f\"Shape of input data (x): {x.shape}\")\n",
    "print(f\"Shape of water depth (WD): {WD.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce09632",
   "metadata": {},
   "source": [
    "<b>Visualizing Input Data Channels</b>\n",
    "\n",
    "Next, we visualize the input data channels (e.g., terrain elevation, slope in x and y directions) using different colormaps. Individual plots are generated for each channel, providing a visual representation of the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the first plot represents terrain, and the subsequent plots use different colormaps\n",
    "colormaps = ['terrain', 'coolwarm', 'coolwarm', 'Blues']\n",
    "\n",
    "# Define titles for each channel\n",
    "channel_titles = ['DEM', 'Slope X', 'Slope Y', f'Water Depth t={t0}']\n",
    "\n",
    "# Plotting the input tensor (x)\n",
    "x_np = x.squeeze(0).cpu().numpy()  # Convert tensor to NumPy array and remove the batch dimension\n",
    "num_channels = x_np.shape[0]  # Number of channels in the input\n",
    "\n",
    "# Plot each channel separately with different colormaps and titles\n",
    "fig, axs = plt.subplots(1, num_channels, figsize=(4 * num_channels, 4))\n",
    "for i in range(num_channels):\n",
    "    axs[i].imshow(x_np[i], cmap=colormaps[i], origin='lower')  # Set origin to lower left and use different colormaps\n",
    "    axs[i].set_title(channel_titles[i])  # Set individual titles for each channel\n",
    "plt.suptitle(f'Test Sample: {data_id}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856390e1",
   "metadata": {},
   "source": [
    "<b>Visualizing Water Depth Over Time</b>\n",
    "\n",
    "Next, the ground truth water depth tensor (WD) is visualized over time. The code generates plots for specific time steps, allowing for the examination of how water depth evolves over the given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the WD tensor\n",
    "WD_np = WD.cpu().numpy()  # Convert tensor to NumPy array\n",
    "\n",
    "# Assuming WD is a sequence of images (96 time steps)\n",
    "num_time_steps = WD_np.shape[0]\n",
    "\n",
    "# Plotting specific time steps (e.g., every 10th time step)\n",
    "time_steps_to_plot = list(range(0, num_time_steps, 10))\n",
    "\n",
    "fig, axs = plt.subplots(1, len(time_steps_to_plot), figsize=(4 * len(time_steps_to_plot), 4))\n",
    "for i, timestep in enumerate(time_steps_to_plot):\n",
    "    axs[i].imshow(WD_np[timestep], cmap='Blues', origin='lower')  # Set origin to lower left\n",
    "    axs[i].set_title(f\"Time Step {timestep + 1}\")\n",
    "plt.suptitle(\"Water Depth (WD) at Specific Time Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f6ac6",
   "metadata": {},
   "source": [
    "<b>Predicting Water Depth and Visualization</b>\n",
    "\n",
    "Finally, the neural network model predicts the water depth (pred_WD) for the selected test sample. The predicted water depth is then visualized over specific time steps, providing a comparison between the ground truth and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f08f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the WD\n",
    "pred_WD = model(x).detach()\n",
    "\n",
    "# Convert the predicted tensor to numpy array\n",
    "pred_WD_np = pred_WD.squeeze(0).cpu().numpy()  # Assuming batch dimension needs to be squeezed\n",
    "\n",
    "# Plotting specific time steps (e.g., every 10th time step)\n",
    "time_steps_to_plot_pred = list(range(0, pred_WD_np.shape[0], 10))\n",
    "\n",
    "fig, axs = plt.subplots(1, len(time_steps_to_plot_pred), figsize=(4 * len(time_steps_to_plot_pred), 4))\n",
    "for i, timestep in enumerate(time_steps_to_plot_pred):\n",
    "    axs[i].imshow(pred_WD_np[timestep], cmap='Blues', origin='lower')  # Assuming 'viridis' colormap, change as needed\n",
    "    axs[i].set_title(f\"Predicted Time Step {timestep + 1}\")\n",
    "plt.suptitle(f'Predicted Water Depth at Specific Time Steps for Test Sample: {data_id}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshaping for the case of 96 timesteps\n",
    "# DEM = scaler_x.inverse_transform(x[0].reshape(4, -1).T.cpu())[:, 0].reshape(64, 64)\n",
    "# real_WD = scaler_y.inverse_transform(WD.reshape(96, -1).cpu()).reshape(96, 64, 64)\n",
    "# pred_WD = scaler_y.inverse_transform(pred_WD.reshape(96, -1).cpu()).reshape(96, 64, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming pred_WD contains the predicted water depths with shape (96, 64, 64)\n",
    "# num_time_steps = 4  # Number of time steps to plot\n",
    "\n",
    "# fig, axs = plt.subplots(1, num_time_steps, figsize=(12, 3))\n",
    "\n",
    "# for i in range(num_time_steps):\n",
    "#     axs[i].imshow(pred_WD[i], cmap='viridis')  # Adjust the colormap as needed\n",
    "#     axs[i].set_title(f\"Time Step {i+1}\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021482fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming pred_WD contains the predicted water depths with shape (96, 64, 64)\n",
    "# num_time_steps = 50  # Number of time steps to plot\n",
    "# sample_index = 0  # Index of the sample to visualize\n",
    "\n",
    "# fig, axs = plt.subplots(5, 10, figsize=(15, 8))\n",
    "\n",
    "# for i in range(5):\n",
    "#     for j in range(10):\n",
    "#         timestep = i * 10 + j\n",
    "#         if timestep < num_time_steps:\n",
    "#             axs[i, j].imshow(pred_WD[timestep], cmap='viridis')  # Adjust the colormap as needed\n",
    "#             axs[i, j].set_title(f\"Time Step {timestep + 1}\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
